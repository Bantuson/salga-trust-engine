---
phase: 03-citizen-reporting-channels
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/models/media.py
  - src/services/__init__.py
  - src/services/storage_service.py
  - src/core/encryption.py
  - src/core/config.py
  - src/models/user.py
  - src/models/ticket.py
  - src/schemas/media.py
  - src/schemas/user.py
  - pyproject.toml
autonomous: true
user_setup:
  - service: aws-s3
    why: "File storage for citizen photos and proof of residence documents"
    env_vars:
      - name: AWS_ACCESS_KEY_ID
        source: "AWS IAM Console -> Users -> Security credentials -> Create access key"
      - name: AWS_SECRET_ACCESS_KEY
        source: "AWS IAM Console -> Users -> Security credentials -> Create access key"
      - name: AWS_REGION
        source: "Choose closest to South Africa (e.g., af-south-1 or eu-west-1)"
      - name: S3_BUCKET_EVIDENCE
        source: "AWS S3 Console -> Create bucket (e.g., salga-evidence-pilot)"
      - name: S3_BUCKET_DOCUMENTS
        source: "AWS S3 Console -> Create bucket (e.g., salga-documents-pilot)"
    dashboard_config:
      - task: "Create two S3 buckets with AES256 default encryption"
        location: "AWS S3 Console -> Create bucket -> Enable server-side encryption"
      - task: "Set CORS policy on evidence bucket to allow browser uploads"
        location: "AWS S3 Console -> Bucket -> Permissions -> CORS"
  - service: encryption-keys
    why: "Field-level encryption for GBV sensitive ticket data"
    env_vars:
      - name: ENCRYPTION_KEY_CURRENT
        source: "Generate with: python -c \"from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())\""
      - name: ENCRYPTION_KEY_PREVIOUS
        source: "Optional - only needed during key rotation. Set to previous ENCRYPTION_KEY_CURRENT value."

must_haves:
  truths:
    - "Media attachment model stores file metadata (S3 key, content type, size) linked to tickets"
    - "S3 storage service can generate presigned POST URLs for direct browser uploads"
    - "S3 storage service can download files from Twilio MediaUrls and upload to S3"
    - "GBV ticket description field is encrypted at rest using Fernet with key rotation support"
    - "User model has verification_status and verified_municipality_id fields for proof of residence"
  artifacts:
    - path: "src/models/media.py"
      provides: "MediaAttachment model with S3 metadata"
      contains: "class MediaAttachment"
    - path: "src/services/storage_service.py"
      provides: "S3 presigned URLs and media upload/download"
      contains: "generate_presigned_post"
    - path: "src/core/encryption.py"
      provides: "EncryptedString SQLAlchemy type with Fernet/MultiFernet"
      contains: "class EncryptedString"
    - path: "src/core/config.py"
      provides: "AWS and encryption settings"
      contains: "AWS_ACCESS_KEY_ID"
  key_links:
    - from: "src/models/media.py"
      to: "src/models/ticket.py"
      via: "ForeignKey ticket_id"
      pattern: "ForeignKey.*tickets"
    - from: "src/core/encryption.py"
      to: "src/models/ticket.py"
      via: "EncryptedString used on description column for GBV tickets"
      pattern: "EncryptedString"
    - from: "src/services/storage_service.py"
      to: "src/core/config.py"
      via: "settings.AWS_ACCESS_KEY_ID"
      pattern: "settings\\.AWS"
---

<objective>
Build the storage infrastructure, media attachment model, and GBV field-level encryption that all other Phase 3 plans depend on.

Purpose: Every Phase 3 feature (WhatsApp media, web uploads, OCR, GBV encryption) needs S3 storage and media tracking. This foundational plan unblocks all parallel work in Wave 2.

Output: MediaAttachment model, S3 StorageService, EncryptedString type, updated config/models
</objective>

<execution_context>
@C:/Users/Bantu/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Bantu/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-citizen-reporting-channels/03-RESEARCH.md
@src/models/base.py
@src/models/ticket.py
@src/models/user.py
@src/core/config.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Media Attachment Model, S3 Storage Service, and Config</name>
  <files>
    src/models/media.py
    src/services/__init__.py
    src/services/storage_service.py
    src/schemas/media.py
    src/core/config.py
    pyproject.toml
  </files>
  <action>
1. Add new dependencies to pyproject.toml:
   - `boto3>=1.34.0` (AWS S3 client)
   - `cryptography>=42.0.0` (Fernet encryption)
   - `Pillow>=10.0.0` (image preprocessing for OCR)
   - `pytesseract>=0.3.10` (OCR engine)
   - `exifread>=3.0.0` (EXIF metadata extraction/stripping)
   - `twilio>=9.0.0` (WhatsApp Business API)
   - `httpx>=0.28.0` (async HTTP client, move from dev to main deps since needed for Twilio media download)

2. Add AWS and encryption settings to `src/core/config.py` Settings class:
   - `AWS_ACCESS_KEY_ID: str = Field(default="", description="AWS access key")`
   - `AWS_SECRET_ACCESS_KEY: str = Field(default="", description="AWS secret key")`
   - `AWS_REGION: str = Field(default="af-south-1", description="AWS region")`
   - `S3_BUCKET_EVIDENCE: str = Field(default="salga-evidence-dev", description="S3 bucket for evidence photos")`
   - `S3_BUCKET_DOCUMENTS: str = Field(default="salga-documents-dev", description="S3 bucket for proof of residence")`
   - `ENCRYPTION_KEY_CURRENT: str = Field(default="", description="Current Fernet encryption key")`
   - `ENCRYPTION_KEY_PREVIOUS: str = Field(default="", description="Previous Fernet key for rotation")`
   - `TWILIO_ACCOUNT_SID: str = Field(default="", description="Twilio account SID")`
   - `TWILIO_AUTH_TOKEN: str = Field(default="", description="Twilio auth token")`
   - `TWILIO_WHATSAPP_NUMBER: str = Field(default="", description="Twilio WhatsApp sender number (whatsapp:+14155238886)")`
   Use empty string defaults so tests work without real credentials.

3. Create `src/models/media.py` — MediaAttachment model:
   - Inherits TenantAwareModel
   - Fields:
     - `ticket_id: Mapped[UUID] = mapped_column(ForeignKey("tickets.id"), nullable=False, index=True)`
     - `file_id: Mapped[str] = mapped_column(String(36), nullable=False, unique=True)` (UUID string for S3 key)
     - `s3_bucket: Mapped[str] = mapped_column(String(100), nullable=False)`
     - `s3_key: Mapped[str] = mapped_column(String(500), nullable=False)`
     - `filename: Mapped[str] = mapped_column(String(255), nullable=False)`
     - `content_type: Mapped[str] = mapped_column(String(100), nullable=False)`
     - `file_size: Mapped[int] = mapped_column(nullable=False)` (bytes)
     - `purpose: Mapped[str] = mapped_column(String(30), nullable=False)` ("evidence" or "proof_of_residence")
     - `source: Mapped[str] = mapped_column(String(20), nullable=False, default="web")` ("web" or "whatsapp")
     - `is_processed: Mapped[bool] = mapped_column(nullable=False, default=False)` (OCR processed flag)
   - __tablename__ = "media_attachments"

4. Create `src/schemas/media.py`:
   - `PresignedUploadRequest(BaseModel)`: filename, content_type, file_size (int), purpose (Literal["evidence", "proof_of_residence"])
   - `PresignedUploadResponse(BaseModel)`: url (str), fields (dict), file_id (str)
   - `MediaAttachmentResponse(BaseModel)`: id, file_id, filename, content_type, file_size, purpose, source, created_at. Use `model_config = ConfigDict(from_attributes=True)`

5. Create `src/services/__init__.py` (empty).

6. Create `src/services/storage_service.py` — StorageService class:
   - `__init__(self)`: Create boto3 S3 client using settings credentials. If AWS_ACCESS_KEY_ID is empty, set `self._client = None` (dev mode without S3).
   - `generate_presigned_post(self, purpose, tenant_id, user_id, file_id, filename, content_type, max_size)` -> dict with url and fields:
     - Determine bucket from purpose ("evidence" -> S3_BUCKET_EVIDENCE, "proof_of_residence" -> S3_BUCKET_DOCUMENTS)
     - Generate S3 key: `{purpose}/{tenant_id}/{file_id}/{filename}`
     - Call `s3_client.generate_presigned_post()` with Content-Type condition and content-length-range [0, max_size], ExpiresIn=900 (15 minutes)
     - Return `{"url": presigned["url"], "fields": presigned["fields"], "file_id": file_id}`
   - `async download_and_upload_media(self, media_url, media_content_type, ticket_id, tenant_id, auth_credentials)` -> dict:
     - Download from Twilio MediaUrl using httpx (async) with Basic auth (TWILIO_ACCOUNT_SID:TWILIO_AUTH_TOKEN)
     - Generate file_id (uuid4), determine extension from content_type
     - S3 key: `evidence/{tenant_id}/{ticket_id}/{file_id}.{extension}`
     - Upload to S3_BUCKET_EVIDENCE with ServerSideEncryption='AES256'
     - Return dict with s3_bucket, s3_key, file_id, content_type, file_size
   - `generate_presigned_get(self, bucket, key, expiry=3600)` -> str: Generate presigned GET URL for downloading files
   - Raise `StorageServiceError` (create custom exception) if S3 client is None (not configured)
  </action>
  <verify>
    python -c "from src.models.media import MediaAttachment; from src.services.storage_service import StorageService; from src.schemas.media import PresignedUploadRequest, PresignedUploadResponse; print('OK')"
  </verify>
  <done>
    MediaAttachment model importable with all fields, StorageService importable with presigned URL methods, media schemas importable, new dependencies in pyproject.toml, AWS/encryption/Twilio settings in config
  </done>
</task>

<task type="auto">
  <name>Task 2: Fernet Field-Level Encryption and User Verification Fields</name>
  <files>
    src/core/encryption.py
    src/models/ticket.py
    src/models/user.py
    src/schemas/user.py
  </files>
  <action>
1. Create `src/core/encryption.py` — EncryptedString SQLAlchemy type:
   - Import Fernet, MultiFernet from cryptography.fernet
   - Import TypeDecorator, String from sqlalchemy
   - Class `EncryptedString(TypeDecorator)`:
     - `impl = String`
     - `cache_ok = True`
     - `__init__(self, length=None)`: Load keys from settings (lazy import to avoid circular). Build MultiFernet from ENCRYPTION_KEY_CURRENT (required) and ENCRYPTION_KEY_PREVIOUS (optional). If ENCRYPTION_KEY_CURRENT is empty, set `self._fernet = None` (allows model import in tests without encryption keys).
     - `process_bind_param(self, value, dialect)`: If value is None return None. If fernet is None, return value as-is (dev/test mode). Encrypt with fernet.encrypt(), return encoded string.
     - `process_result_value(self, value, dialect)`: If value is None return None. If fernet is None, return value as-is. Decrypt with fernet.decrypt(), return decoded string.
   - IMPORTANT: When ENCRYPTION_KEY_CURRENT is empty (tests/dev), EncryptedString stores values in plaintext. This is intentional - encryption only active when keys configured.

2. Update `src/models/ticket.py`:
   - Import EncryptedString from src.core.encryption
   - Add a new field `encrypted_description: Mapped[str | None] = mapped_column(EncryptedString(5000), nullable=True)` — Used ONLY for GBV tickets (is_sensitive=True). For GBV tickets, the sensitive description goes into encrypted_description, while the plain description holds a generic non-sensitive summary (e.g., "GBV incident report").
   - Add `media_urls: Mapped[str | None] = mapped_column(Text, nullable=True)` — JSON-serialized list of media file_ids associated with ticket. This is a denormalized convenience field; MediaAttachment table is the source of truth.

3. Update `src/models/user.py`:
   - Add `verification_status: Mapped[str] = mapped_column(String(20), nullable=False, default="unverified")` — Values: "unverified", "pending", "verified", "rejected"
   - Add `verified_address: Mapped[str | None] = mapped_column(String(500), nullable=True)` — OCR-extracted address from proof of residence
   - Add `verification_document_id: Mapped[str | None] = mapped_column(String(36), nullable=True)` — file_id of uploaded proof of residence in S3
   - Add `verified_at: Mapped[datetime | None] = mapped_column(DateTime(timezone=True), nullable=True)` — When verification was approved

4. Update `src/schemas/user.py`:
   - Add `UserVerificationResponse(BaseModel)`: verification_status, verified_address (optional), verified_at (optional). ConfigDict(from_attributes=True)
   - Add `UserVerificationRequest(BaseModel)`: document_file_id (str) — The file_id of the uploaded proof of residence document to process via OCR
  </action>
  <verify>
    python -c "from src.core.encryption import EncryptedString; from src.models.ticket import Ticket; from src.models.user import User; print('Ticket has encrypted_description:', hasattr(Ticket, 'encrypted_description')); print('User has verification_status:', hasattr(User, 'verification_status'))"
  </verify>
  <done>
    EncryptedString type works in plaintext mode (no keys) and encrypted mode (with keys). Ticket model has encrypted_description for GBV data and media_urls field. User model has verification_status, verified_address, verification_document_id, and verified_at fields. User schemas include verification request/response.
  </done>
</task>

</tasks>

<verification>
- All new models importable without errors
- StorageService importable and raises StorageServiceError when unconfigured
- EncryptedString works in plaintext mode (empty encryption key)
- Config has all new settings (AWS, encryption, Twilio)
- pyproject.toml has all new dependencies
- No import errors in any existing module
- `python -c "from src.main import app; print('App OK')"` succeeds
</verification>

<success_criteria>
- MediaAttachment model created with S3 metadata fields and ticket FK
- StorageService generates presigned POST and GET URLs
- EncryptedString TypeDecorator encrypts/decrypts with Fernet
- User model has verification fields
- Ticket model has encrypted_description for GBV
- All existing tests still pass (no regressions)
</success_criteria>

<output>
After completion, create `.planning/phases/03-citizen-reporting-channels/03-01-SUMMARY.md`
</output>
