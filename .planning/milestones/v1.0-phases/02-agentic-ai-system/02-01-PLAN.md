---
phase: 02-agentic-ai-system
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - src/core/language.py
  - src/core/conversation.py
  - src/models/ticket.py
  - src/models/__init__.py
  - src/schemas/ticket.py
  - src/schemas/__init__.py
  - alembic/versions/02_01_ticket_model.py
  - tests/test_language.py
  - tests/test_conversation.py
autonomous: true

must_haves:
  truths:
    - "Language detection correctly identifies English, isiZulu, and Afrikaans from text input"
    - "Short messages (<20 chars) fall back to user preferred language instead of guessing"
    - "Conversation state persists across multiple turns via Redis with TTL expiry"
    - "Ticket model stores category, description, location, severity, and links to user/municipality"
    - "GBV tickets use separate Redis namespace from municipal tickets"
  artifacts:
    - path: "src/core/language.py"
      provides: "Language detection wrapper using lingua-py"
      contains: "LanguageDetector"
    - path: "src/core/conversation.py"
      provides: "Redis-backed conversation state manager"
      contains: "ConversationManager"
    - path: "src/models/ticket.py"
      provides: "Ticket SQLAlchemy model with PostGIS location"
      contains: "class Ticket"
    - path: "src/schemas/ticket.py"
      provides: "Pydantic schemas for ticket data"
      contains: "TicketCreate"
  key_links:
    - from: "src/core/language.py"
      to: "lingua-language-detector"
      via: "lingua.LanguageDetectorBuilder"
      pattern: "LanguageDetectorBuilder"
    - from: "src/core/conversation.py"
      to: "redis"
      via: "redis.asyncio connection"
      pattern: "redis\\.asyncio"
    - from: "src/models/ticket.py"
      to: "src/models/base.py"
      via: "TenantAwareModel inheritance"
      pattern: "class Ticket\\(TenantAwareModel\\)"
---

<objective>
Build the foundational infrastructure that all Phase 2 agent plans depend on: deterministic language detection, Redis-backed conversation state management, and the Ticket data model.

Purpose: Agents need language detection to respond in the correct language (AI-06, PLAT-04), conversation state to maintain multi-turn intake context, and a Ticket model as the output target for completed intake sessions.

Output: Language detector module, conversation manager, Ticket model + migration, Ticket Pydantic schemas, unit tests for language detection and conversation state.
</objective>

<execution_context>
@C:/Users/Bantu/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Bantu/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-agentic-ai-system/02-RESEARCH.md

# Phase 1 foundation context
@src/core/config.py
@src/core/database.py
@src/models/base.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install Phase 2 dependencies and create language detection module</name>
  <files>
    pyproject.toml
    src/core/language.py
    tests/test_language.py
  </files>
  <action>
1. Add Phase 2 dependencies to pyproject.toml `dependencies` list:
   - `crewai[tools]>=0.99.0` (agent framework)
   - `lingua-language-detector>=2.0.0` (language detection)
   - `tiktoken` (token counting for cost management)
   Add to `[project.optional-dependencies] dev`:
   - `fakeredis>=2.21.0` (Redis mock for tests)

   Do NOT add nemoguardrails yet (Plan 02-04) or langfuse (post-MVP observability).

2. Create `src/core/language.py`:
   - Import `from lingua import Language, LanguageDetectorBuilder`
   - Create singleton `LanguageDetector` class:
     - Build detector for exactly 3 languages: Language.ENGLISH, Language.ZULU, Language.AFRIKAANS
     - Use `LanguageDetectorBuilder.from_languages(...).with_minimum_relative_distance(0.25).build()`
     - Map lingua Language enum to ISO codes: {"ENGLISH": "en", "ZULU": "zu", "AFRIKAANS": "af"}
   - Method `detect(text: str, fallback: str = "en") -> str`:
     - If text length < 20 chars, return fallback (short text unreliable per research Pitfall 4)
     - Call `detector.detect_language_of(text)`
     - If None (undetectable), return fallback
     - Get confidence via `detector.compute_language_confidence_values(text)`
     - If highest confidence < 0.7, return fallback
     - Return mapped ISO code ("en", "zu", or "af")
   - Method `detect_with_confidence(text: str) -> tuple[str, float]`:
     - Returns (language_code, confidence_score) for logging/debugging
   - Module-level singleton: `language_detector = LanguageDetector()`

3. Create `tests/test_language.py` (unit tests, no markers needed):
   - Test English detection: "There is a water pipe burst on Main Street" -> "en"
   - Test isiZulu detection: "Amanzi ami ayaphuma endlini yami" -> "zu"
   - Test Afrikaans detection: "My water lek by die pyp naby die winkel" -> "af"
   - Test short text fallback: "ok" with fallback="zu" -> "zu"
   - Test empty string: "" -> fallback
   - Test detect_with_confidence returns tuple
   - Test fallback for ambiguous text

Run `pip install -e ".[dev]"` after dependency changes. Run `pytest tests/test_language.py -v` to verify.
  </action>
  <verify>
    `pytest tests/test_language.py -v` passes all tests.
    `python -c "from src.core.language import language_detector; print(language_detector.detect('Hello world'))"` outputs "en".
  </verify>
  <done>
    Language detector correctly identifies EN/ZU/AF with confidence thresholds, falls back gracefully for short/ambiguous text, singleton available for import.
  </done>
</task>

<task type="auto">
  <name>Task 2: Conversation state manager and Ticket model with migration</name>
  <files>
    src/core/conversation.py
    src/models/ticket.py
    src/models/__init__.py
    src/schemas/ticket.py
    src/schemas/__init__.py
    alembic/versions/02_01_ticket_model.py
    tests/test_conversation.py
  </files>
  <action>
1. Create `src/core/conversation.py`:
   - Import `redis.asyncio as redis` and `pydantic.BaseModel`
   - Define `ConversationState(BaseModel)`:
     - user_id: str
     - session_id: str
     - tenant_id: str
     - language: str = "en"
     - category: str | None = None (municipal / gbv)
     - turns: list[dict] = [] (each: {"role": "user"|"agent", "content": str, "timestamp": float})
     - collected_data: dict = {} (partial ticket data as it's gathered)
     - created_at: float
     - max_turns: int = 20 (safety limit per research Pitfall 5)
   - Define `ConversationManager`:
     - `__init__(self, redis_url: str, default_ttl: int = 3600)`: Create async redis connection
     - Namespace constants: MUNICIPAL_PREFIX = "conv:municipal:", GBV_PREFIX = "conv:gbv:"
     - `_get_key(self, user_id: str, session_id: str, is_gbv: bool = False) -> str`:
       Use GBV_PREFIX for GBV conversations, MUNICIPAL_PREFIX otherwise (research: separate namespaces)
     - `async get_state(self, user_id, session_id, is_gbv=False) -> ConversationState | None`:
       Get from Redis, deserialize with model_validate_json
     - `async save_state(self, state, is_gbv=False)`:
       Serialize with model_dump_json, setex with TTL
     - `async append_turn(self, user_id, session_id, role, content, is_gbv=False) -> ConversationState`:
       Get state, append turn with timestamp, check max_turns (raise ValueError if exceeded), save, return updated state
     - `async clear_session(self, user_id, session_id, is_gbv=False)`:
       Delete key from Redis (used after GBV ticket creation per research Pitfall 3)
     - `async create_session(self, user_id, session_id, tenant_id, language, is_gbv=False) -> ConversationState`:
       Create new ConversationState, save to Redis, return it

2. Create `src/models/ticket.py`:
   - Import TenantAwareModel from src.models.base
   - Define TicketCategory enum (StrEnum): WATER, ROADS, ELECTRICITY, WASTE, SANITATION, GBV, OTHER
   - Define TicketStatus enum (StrEnum): OPEN, IN_PROGRESS, ESCALATED, RESOLVED, CLOSED
   - Define TicketSeverity enum (StrEnum): LOW, MEDIUM, HIGH, CRITICAL
   - Define `class Ticket(TenantAwareModel)`:
     - __tablename__ = "tickets"
     - tracking_number: Mapped[str] = mapped_column(String(20), unique=True, nullable=False)
       Use format: "TKT-{YYYYMMDD}-{6_random_hex}" generated via default factory
     - category: Mapped[str] = mapped_column(String(20), nullable=False) (from TicketCategory)
     - description: Mapped[str] = mapped_column(Text, nullable=False)
     - latitude: Mapped[float | None] = mapped_column(Float, nullable=True)
     - longitude: Mapped[float | None] = mapped_column(Float, nullable=True)
     - address: Mapped[str | None] = mapped_column(String(500), nullable=True)
     - severity: Mapped[str] = mapped_column(String(20), default=TicketSeverity.MEDIUM)
     - status: Mapped[str] = mapped_column(String(20), default=TicketStatus.OPEN)
     - language: Mapped[str] = mapped_column(String(5), default="en")
     - user_id: Mapped[UUID] = mapped_column(ForeignKey("users.id"), nullable=False)
     - assigned_to: Mapped[UUID | None] = mapped_column(ForeignKey("users.id"), nullable=True)
     - resolved_at: Mapped[datetime | None]
     - is_sensitive: Mapped[bool] = mapped_column(default=False) (True for GBV tickets)
     - Note: Do NOT use PostGIS ST_Point yet - use separate lat/lng columns. PostGIS geospatial queries come in Phase 4.

3. Update `src/models/__init__.py` to export Ticket and its enums.

4. Create `src/schemas/ticket.py`:
   - TicketCreate(BaseModel): category, description, latitude?, longitude?, address?, severity?, language?
   - TicketResponse(BaseModel): id, tracking_number, category, description, latitude, longitude, address, severity, status, language, user_id, is_sensitive, created_at
     Use model_config = ConfigDict(from_attributes=True)
   - TicketUpdate(BaseModel): description?, severity?, status? (all Optional)
   - TicketData(BaseModel): Structured output schema for agent intake
     category: str (validated against TicketCategory values)
     description: str (min_length=20)
     latitude: float | None
     longitude: float | None
     address: str | None
     severity: str (validated against TicketSeverity values)

5. Update `src/schemas/__init__.py` to export ticket schemas.

6. Create Alembic migration manually (same pattern as 01-01):
   - File: `alembic/versions/02_01_ticket_model.py` with proper revision ID
   - Create `tickets` table with all columns matching the model
   - Add indexes on: tenant_id, category, status, user_id, tracking_number (unique)
   - Add foreign keys to users table

7. Create `tests/test_conversation.py` (unit tests using fakeredis):
   - Test create_session creates and returns ConversationState
   - Test get_state returns None for nonexistent session
   - Test append_turn adds turn and returns updated state
   - Test max_turns enforcement raises ValueError
   - Test clear_session removes state
   - Test GBV namespace separation (GBV and municipal keys don't collide)
   - Use fakeredis.aioredis.FakeRedis for mocking (no real Redis needed)

Run `pytest tests/test_conversation.py -v` to verify.
  </action>
  <verify>
    `pytest tests/test_conversation.py -v` passes all tests.
    `python -c "from src.models.ticket import Ticket, TicketCategory; print(list(TicketCategory))"` shows all categories.
    `python -c "from src.schemas.ticket import TicketCreate, TicketResponse; print('OK')"` imports without error.
    Migration file exists in alembic/versions/.
  </verify>
  <done>
    Conversation manager handles session lifecycle with separate GBV/municipal namespaces. Ticket model defines all fields needed for municipal and GBV reports. Ticket schemas validate intake data. Migration ready for PostgreSQL.
  </done>
</task>

</tasks>

<verification>
1. `pytest tests/test_language.py tests/test_conversation.py -v` -- all tests pass
2. `python -c "from src.core.language import language_detector; print(language_detector.detect('Amanzi ami ayaphuma'))"` -- outputs "zu"
3. `python -c "from src.core.conversation import ConversationManager; print('OK')"` -- imports clean
4. `python -c "from src.models.ticket import Ticket, TicketCategory, TicketStatus; print('OK')"` -- imports clean
5. All existing Phase 1 tests still pass: `pytest tests/ -v --ignore=tests/test_language.py --ignore=tests/test_conversation.py`
</verification>

<success_criteria>
- Language detector identifies EN/ZU/AF correctly with >= 0.7 confidence threshold
- Short text (<20 chars) falls back to preferred language
- Conversation state manager creates, reads, updates, and deletes sessions via Redis
- GBV conversations use separate Redis namespace from municipal conversations
- Ticket model has all required fields (category, description, location, severity, status, user link)
- Ticket Pydantic schemas validate agent intake output
- All unit tests pass, all Phase 1 tests still pass
</success_criteria>

<output>
After completion, create `.planning/phases/02-agentic-ai-system/02-01-SUMMARY.md`
</output>
