---
phase: 10.3-crewai-agent-rebuild-and-llm-evaluation-framework
plan: 02
subsystem: agents/evals
tags: [deepeval, trajectory-eval, llm-judge, eval-scenarios, gbv-firewall, popia]

# Dependency graph
requires:
  - phase: 10.3-01
    provides: deepeval installed, tests/evals/ package scaffold, reports/ directory

provides:
  - 25 eval scenarios across 4 specialist agents (Auth:7, Municipal:7, GBV:5, TicketStatus:6)
  - Trajectory evaluation harness via deepeval ToolCorrectnessMetric
  - Content signal checker (required/forbidden word validation)
  - Timestamped JSON eval report saver with GBV metadata-only stripping
  - Full agent evaluation loop (evaluate_agent)
  - LLM judge rubrics for all 4 specialist agents
  - RUBRIC_MAP registry and format_rubric() helper

affects: [10.3-03, 10.3-04, 10.3-05, 10.3-06, 10.3-07, 10.3-08, 10.3-09]

# Tech tracking
tech-stack:
  added: []
  patterns:
    - deepeval ToolCorrectnessMetric with threshold=0.8 for tool sequence evaluation
    - metadata_only=True pattern for GBV scenarios (POPIA data minimization in eval reports)
    - RUBRIC_MAP + format_rubric() for programmatic LLM judge access
    - check_content_signals() case-insensitive required/forbidden word matching
    - save_eval_report() strips actual_output for metadata_only scenarios before writing JSON

key-files:
  created:
    - tests/evals/scenarios/auth_scenarios.py
    - tests/evals/scenarios/municipal_scenarios.py
    - tests/evals/scenarios/ticket_status_scenarios.py
    - tests/evals/scenarios/gbv_scenarios.py
    - tests/evals/conftest.py
    - tests/evals/trajectory_evals.py
    - tests/evals/judge_rubrics.py
  modified: []

key-decisions:
  - "25 total scenarios (Auth:7, Municipal:7, GBV:5, TicketStatus:6) — within 20-28 target range"
  - "GBV scenarios: metadata_only=True strips ALL response content from eval reports (POPIA compliance)"
  - "All GBV scenarios include 10111 and 0800 150 150 in expected_content_signals (adversarial/edge excepted — forbidden signals cover this)"
  - "ToolCorrectnessMetric threshold=0.8 — partial credit for near-correct tool sequences"
  - "evaluate_agent() combines trajectory eval AND content signals — both must pass for scenario pass"
  - "GBV judge rubric has 7 REQUIRED criteria (all critical) vs 5+1 RECOMMENDED for auth/ticket status"
  - "RUBRIC_MAP uses string keys matching agent names in crew_server.py routing"

requirements-completed: [AI-05, AI-06, AI-07]

# Metrics
duration: 12min
completed: 2026-02-25
---

# Phase 10.3 Plan 02: LLM Evaluation Framework Summary

**25 eval scenarios across 4 agents, trajectory eval harness with deepeval ToolCorrectnessMetric, and judge rubrics covering persona, language, tool sequence, and no-leakage for all specialist agents**

## Performance

- **Duration:** 12 min
- **Started:** 2026-02-25T14:06:36Z
- **Completed:** 2026-02-25T14:18:32Z
- **Tasks:** 2
- **Files modified:** 7 (all created)

## Accomplishments

- Created 4 scenario files: 25 total eval scenarios covering happy path, edge cases, language switch (EN/ZU/AF), and adversarial inputs for all 4 specialist agents
- All 5 GBV scenarios have `metadata_only=True` — eval reports never log conversation content (POPIA data minimization)
- Emergency numbers (10111, 0800 150 150) in expected_content_signals for all GBV scenarios that should produce them
- Built trajectory eval harness using deepeval `ToolCorrectnessMetric` with threshold=0.8
- Content signal checker validates required words (case-insensitive) and forbidden words (prevents leakage, adversarial pass)
- `save_eval_report()` strips `actual_output` for `metadata_only=True` scenarios before writing JSON — GBV firewall extends to eval artifacts
- `evaluate_agent()` full loop: run scenarios, check signals, save timestamped report, return summary
- Judge rubrics for all 4 specialist agents with REQUIRED/RECOMMENDED criteria annotations
- GBV rubric has 7 REQUIRED criteria (unique trauma_informed + emergency_numbers + no_pii_logging)
- `RUBRIC_MAP` dict and `format_rubric()` helper for programmatic rubric access

## Task Commits

Each task was committed atomically:

1. **Task 1: Define evaluation scenarios for all 4 agents** - `ee10df4` (feat)
2. **Task 2: Build trajectory eval harness and judge rubrics** - `fd4077c` (feat)

## Files Created

- `tests/evals/scenarios/auth_scenarios.py` — AUTH_SCENARIOS (7 scenarios)
- `tests/evals/scenarios/municipal_scenarios.py` — MUNICIPAL_SCENARIOS (7 scenarios)
- `tests/evals/scenarios/ticket_status_scenarios.py` — TICKET_STATUS_SCENARIOS (6 scenarios)
- `tests/evals/scenarios/gbv_scenarios.py` — GBV_SCENARIOS (5 scenarios, all metadata_only=True)
- `tests/evals/conftest.py` — fake API keys, tool_calls_captured fixture, make_step_callback factory
- `tests/evals/trajectory_evals.py` — run_trajectory_eval, check_content_signals, save_eval_report, evaluate_agent
- `tests/evals/judge_rubrics.py` — AUTH/MUNICIPAL/GBV/TICKET_STATUS rubrics, RUBRIC_MAP, format_rubric

## Scenario Summary

| Agent | Count | Languages | Adversarial | metadata_only |
|-------|-------|-----------|-------------|---------------|
| Auth | 7 | EN, ZU, AF | 1 (prompt injection) | False |
| Municipal | 7 | EN, ZU, AF | 1 (system injection) | False |
| GBV | 5 | EN, ZU | 1 (extract SAPS info) | True (all) |
| Ticket Status | 6 | EN, ZU, AF | 1 (SQL injection) | False |
| **Total** | **25** | | **4** | **5 GBV** |

## Judge Rubric Criteria

| Agent | REQUIRED Criteria | RECOMMENDED |
|-------|-------------------|-------------|
| Auth | persona, language, tool_sequence, no_leakage | single_question |
| Municipal | persona, language, collected_info, tool_sequence, no_leakage | single_question |
| GBV | persona, language, trauma_informed, emergency_numbers, no_pii_logging, tool_sequence, no_leakage | (none) |
| Ticket Status | persona, language, tracking_number_handling, tool_sequence, no_leakage | single_question |

## Deviations from Plan

None — plan executed exactly as written.

## Issues Encountered

- Windows LF→CRLF line ending warnings on git commit — cosmetic only, expected on this machine

## Self-Check: PASSED

Files exist:
- FOUND: tests/evals/scenarios/auth_scenarios.py
- FOUND: tests/evals/scenarios/municipal_scenarios.py
- FOUND: tests/evals/scenarios/ticket_status_scenarios.py
- FOUND: tests/evals/scenarios/gbv_scenarios.py
- FOUND: tests/evals/conftest.py
- FOUND: tests/evals/trajectory_evals.py
- FOUND: tests/evals/judge_rubrics.py

Commits exist:
- FOUND: ee10df4 (Task 1 — scenario definitions)
- FOUND: fd4077c (Task 2 — trajectory harness + judge rubrics)

---
*Phase: 10.3-crewai-agent-rebuild-and-llm-evaluation-framework*
*Completed: 2026-02-25*
