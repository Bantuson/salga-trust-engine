---
phase: 10.3-crewai-agent-rebuild-and-llm-evaluation-framework
plan: 08
type: execute
wave: 5
depends_on: ["10.3-07"]
files_modified:
  - tests/evals/trajectory_evals.py
  - tests/evals/run_evals.py
  - tests/agents/test_full_pipeline.py
autonomous: false
requirements:
  - AI-01
  - AI-02
  - AI-03
  - AI-04
  - AI-05
  - AI-06
  - AI-07

must_haves:
  truths:
    - "Full pipeline test verifies HTTP -> crew_server -> IntakeFlow -> specialist -> response"
    - "Trajectory eval harness executes against real agent with mocked tools"
    - "Eval report generated with pass/fail per scenario, saved to tests/evals/reports/"
    - "All existing tests still pass (regression check)"
    - "Manual Streamlit spot-check demonstrates conversational quality"
  artifacts:
    - path: "tests/agents/test_full_pipeline.py"
      provides: "Integration tests for the full agent pipeline"
      contains: "class TestFullPipeline"
    - path: "tests/evals/run_evals.py"
      provides: "CLI entry point for running trajectory evals"
  key_links:
    - from: "tests/agents/test_full_pipeline.py"
      to: "src/api/v1/crew_server.py"
      via: "TestClient HTTP calls through full pipeline"
      pattern: "TestClient\\(crew_app\\)"
    - from: "tests/evals/run_evals.py"
      to: "tests/evals/trajectory_evals.py"
      via: "evaluate_agent function"
      pattern: "evaluate_agent"
---

<objective>
Run integration tests, trajectory evals, and manual Streamlit verification for the complete rebuilt agent system.

Purpose: This is the final quality gate. Integration tests verify the full HTTP -> Flow -> Crew pipeline structurally. Trajectory evals (if real LLM keys available) verify tool call correctness. The manual Streamlit checkpoint allows the user to conversationally verify agent quality before the phase is considered complete. This plan has a checkpoint because the user must verify conversational quality via Streamlit.

Output: Green integration tests, eval reports, user approval of agent quality
</objective>

<execution_context>
@C:/Users/Bantu/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Bantu/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/10.3-crewai-agent-rebuild-and-llm-evaluation-framework/10.3-RESEARCH.md
@.planning/phases/10.3-crewai-agent-rebuild-and-llm-evaluation-framework/10.3-07-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Full pipeline integration tests and eval runner</name>
  <files>
    tests/agents/test_full_pipeline.py
    tests/evals/run_evals.py
  </files>
  <action>
    1. `tests/agents/test_full_pipeline.py` — Integration tests for complete pipeline:

       **Section 1: End-to-End Pipeline (mocked crews)**
       - `test_chat_new_user_routes_to_auth` — POST /api/v1/chat with new phone routes through IntakeFlow to auth
       - `test_chat_authenticated_municipal` — POST /api/v1/chat with active session and municipal message routes to municipal
       - `test_chat_authenticated_ticket_status` — Active session + "status of TKT-xxx" routes to ticket_status
       - `test_chat_gbv_message` — GBV-related message routes to GBV crew
       - `test_chat_gbv_confirmation_flow` — gbv_pending_confirm state handles YES/NO correctly
       - `test_chat_response_shape` — Every response has reply, agent_name, session_status, language
       - `test_chat_sanitized_output` — Response never contains "Final Answer:", "Thought:", "Action:"

       **Section 2: Routing Consistency**
       - `test_routing_phase_persists` — After auth completes, routing_phase updated in conversation state
       - `test_pending_intent_restored` — After auth, citizen's original intent (e.g., "municipal") is used for next routing
       - `test_session_reset_clears_state` — POST /api/v1/session/reset clears conversation state

       **Section 3: Error Handling**
       - `test_chat_crew_error_returns_fallback` — If crew raises exception, fallback message returned (not 500)
       - `test_chat_fallback_gbv_has_emergency_numbers` — GBV fallback includes 10111, 0800 150 150

       **Section 4: Regression Check**
       - `test_existing_test_suite_passes` — Run: `pytest tests/ -x --ignore=tests/evals/ --ignore=tests/agents/ -q` to verify no regressions in existing (non-agent) tests. This is a subprocess call from within the test.
       - NOTE: Some old agent tests (test_manager_crew.py etc. in tests/ root) may fail because they reference agents_old. That's expected — the old tests were testing old architecture. The new tests in tests/agents/ replace them.

       **Mock pattern:**
       - Mock all specialist crew kickoff methods to return predictable dicts
       - Use TestClient(crew_app) for HTTP-level testing
       - Mock ConversationManager for state manipulation

    2. `tests/evals/run_evals.py` — CLI entry point for running trajectory evals:
       ```python
       """CLI entry point for running trajectory evals against live agents.

       Usage:
           python -m tests.evals.run_evals --agent auth
           python -m tests.evals.run_evals --agent all
           python -m tests.evals.run_evals --agent auth --dry-run  # Show scenarios only

       Requires real LLM API keys (DEEPSEEK_API_KEY, OPENAI_API_KEY).
       NOT run in CI — only for manual evaluation.
       """
       import argparse
       import json
       import sys
       from datetime import datetime
       from tests.evals.trajectory_evals import evaluate_agent
       from tests.evals.scenarios.auth_scenarios import AUTH_SCENARIOS
       from tests.evals.scenarios.municipal_scenarios import MUNICIPAL_SCENARIOS
       from tests.evals.scenarios.ticket_status_scenarios import TICKET_STATUS_SCENARIOS
       from tests.evals.scenarios.gbv_scenarios import GBV_SCENARIOS

       SCENARIO_MAP = {
           "auth": AUTH_SCENARIOS,
           "municipal": MUNICIPAL_SCENARIOS,
           "ticket_status": TICKET_STATUS_SCENARIOS,
           "gbv": GBV_SCENARIOS,
       }

       def main():
           parser = argparse.ArgumentParser(description="Run agent trajectory evals")
           parser.add_argument("--agent", choices=["auth", "municipal", "ticket_status", "gbv", "all"], required=True)
           parser.add_argument("--dry-run", action="store_true", help="Show scenarios without running")
           args = parser.parse_args()

           agents = list(SCENARIO_MAP.keys()) if args.agent == "all" else [args.agent]

           for agent_name in agents:
               scenarios = SCENARIO_MAP[agent_name]
               if args.dry_run:
                   print(f"\n{agent_name}: {len(scenarios)} scenarios")
                   for s in scenarios:
                       print(f"  - {s['name']} (lang={s['language']})")
                   continue

               print(f"\nRunning {agent_name} evals ({len(scenarios)} scenarios)...")
               # The actual run_scenario_fn would use the real crew with mocked external services
               # This is a placeholder — the real implementation calls the crew and captures tool calls
               results = evaluate_agent(agent_name, scenarios, _make_runner(agent_name))
               print(f"  Results: {results['summary']}")

       def _make_runner(agent_name):
           """Create a scenario runner for the given agent."""
           def run_scenario(scenario):
               # Import and run the actual crew
               # Capture tool calls via step_callback
               # Return (actual_output, actual_tool_calls)
               raise NotImplementedError(f"Live eval runner for {agent_name} — requires real LLM keys")
           return run_scenario

       if __name__ == "__main__":
           main()
       ```

       NOTE: The live eval runners are intentionally NotImplementedError. They require real LLM API keys and are meant to be filled in during actual eval runs. The dry-run mode works immediately for listing scenarios.
  </action>
  <verify>
    <automated>pytest tests/agents/test_full_pipeline.py -x --no-header -q && python -m tests.evals.run_evals --agent all --dry-run</automated>
  </verify>
  <done>12+ integration tests covering full pipeline routing, sanitization, error handling, GBV confirmation. Eval runner CLI works in dry-run mode listing all scenarios. Existing non-agent tests pass (regression check).</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Quick manual Streamlit smoke-test before automated eval in Plan 09</name>
  <action>
    Quick smoke-test of rebuilt agent system (full automated Playwright+Claude-judge eval runs in Plan 09):
    1. Start the crew server: `uvicorn src.api.v1.crew_server:crew_app --port 8001`
    2. Start the Streamlit dashboard: `streamlit run streamlit_dashboard/app.py`
    3. Send one message ("Hi, I want to report a pothole") and verify Gugu responds
    4. Verify NO delegation artifacts in response
    5. Run full test suite: `pytest tests/agents/ -x -q` — all should pass

    NOTE: Requires real DEEPSEEK_API_KEY and OPENAI_API_KEY for live testing.
    Without API keys, verify structural correctness via test suite.
    The full dynamic eval loop (all scenarios, all agents, rubric scoring) runs in Plan 09 via Playwright+Claude-judge.
  </action>
  <verify>User types "approved" or describes issues</verify>
  <done>User confirms basic agent functionality via quick Streamlit smoke-test. Full eval deferred to Plan 09.</done>
</task>

</tasks>

<verification>
- `pytest tests/agents/ -x -q` — full agent test suite green
- `pytest tests/agents/test_full_pipeline.py -x -q` — integration tests green
- `python -m tests.evals.run_evals --agent all --dry-run` — lists all scenarios
- Manual Streamlit verification approved by user
</verification>

<success_criteria>
- Full pipeline: HTTP -> crew_server -> IntakeFlow -> specialist Crew -> response works end-to-end
- No delegation artifacts in any agent response
- Gugu persona maintained across all agents
- Emergency numbers present in GBV responses
- Eval framework ready for execution (dry-run works, live mode needs API keys)
- All tests/agents/ tests pass
- Existing non-agent tests still pass (no regressions)
- User approves agent quality via Streamlit checkpoint
</success_criteria>

<output>
After completion, create `.planning/phases/10.3-crewai-agent-rebuild-and-llm-evaluation-framework/10.3-08-SUMMARY.md`
</output>
