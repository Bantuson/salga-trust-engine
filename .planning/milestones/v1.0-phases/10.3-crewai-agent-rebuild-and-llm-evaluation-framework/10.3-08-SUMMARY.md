---
phase: 10.3-crewai-agent-rebuild-and-llm-evaluation-framework
plan: "08"
subsystem: testing
tags: [crewai, deepseek, gpt-4o-mini, trajectory-evals, integration-tests, llm-evaluation]

# Dependency graph
requires:
  - phase: 10.3-07
    provides: IntakeFlow with @router dispatch to all 4 specialist crews
provides:
  - Full pipeline integration tests (HTTP -> crew_server -> IntakeFlow -> specialist -> response)
  - Trajectory eval harness with 25 scenarios across 4 agents
  - Eval runner CLI (dry-run and live modes)
  - LLM assignment fix: all specialist agents use DeepSeek V3.2 (gpt-4o-mini fails with long prompts)
affects:
  - 10.3-09 (Playwright+Claude-judge dynamic eval loop depends on this eval framework)

# Tech tracking
tech-stack:
  added: []
  patterns:
    - "LLM selection based on empirical eval: DeepSeek for long-backstory specialist agents, gpt-4o-mini for short classification only"
    - "Trajectory eval harness: scenario-based testing with expected tool call sequences"
    - "Eval runner CLI with --dry-run mode for CI-safe scenario listing"
    - "TestClient(crew_app) for HTTP-level integration testing of full pipeline"

key-files:
  created:
    - tests/agents/test_full_pipeline.py
    - tests/evals/run_evals.py
  modified:
    - src/agents/crews/auth_crew.py
    - src/agents/crews/municipal_crew.py
    - src/agents/crews/ticket_status_crew.py
    - src/agents/llm.py
    - tests/agents/test_auth_crew.py

key-decisions:
  - "Switch auth/municipal/ticket_status from gpt-4o-mini to DeepSeek V3.2 — per-agent trajectory evals proved gpt-4o-mini ignores backstory/tools in 150-300 line prompts (0/3 tool calls correct)"
  - "get_routing_llm() (gpt-4o-mini) kept ONLY for IntakeFlow intent classification: short prompt <50 tokens, no tools, simple 5-way classification"
  - "Eval runner live mode uses NotImplementedError placeholder — requires real LLM API keys and is filled in during manual eval runs"
  - "19 integration tests in test_full_pipeline.py cover full HTTP pipeline with mocked crews — structural correctness without real LLM calls"

patterns-established:
  - "LLM assignment via eval: never assume from research; always validate with per-agent trajectory evals against real prompts"
  - "TestClient(crew_app) pattern for full pipeline integration tests"
  - "Dry-run mode mandatory for eval CLIs — allows CI-safe scenario listing without API keys"

requirements-completed:
  - AI-01
  - AI-02
  - AI-03
  - AI-04
  - AI-05
  - AI-06
  - AI-07

# Metrics
duration: 15min
completed: 2026-02-26
---

# Phase 10.3 Plan 08: Integration Tests, Eval Framework, and LLM Fix Summary

**19 pipeline integration tests + 25-scenario eval framework + empirically-driven LLM switch from gpt-4o-mini to DeepSeek for all 4 specialist agents**

## Performance

- **Duration:** ~15 min (this session: LLM switch + verification)
- **Started:** 2026-02-26T07:02:34Z
- **Completed:** 2026-02-26T07:17:52Z
- **Tasks:** 2 (Task 1 from previous session, LLM fix in this session)
- **Files modified:** 7

## Accomplishments

- Full pipeline integration tests (19 tests): HTTP -> crew_server -> IntakeFlow -> specialist -> response, covering routing, sanitization, GBV confirmation gate, error handling, session reset
- Trajectory eval harness with 25 scenarios across 4 agents (auth: 7, municipal: 7, ticket_status: 6, GBV: 5) with eval runner CLI supporting --dry-run and --agent flags
- LLM switch: Auth, MunicipalIntake, and TicketStatus crews moved from gpt-4o-mini to DeepSeek V3.2 after empirical evals showed 3/4 agents failing (gpt-4o-mini ignores backstory/tools in 150-300 line prompts)
- All 274 unit tests pass after the switch

## Task Commits

Each task was committed atomically:

1. **Task 1: Full pipeline integration tests and eval runner CLI** - `e58167b` (test)
2. **LLM Fix: Switch 3 crew files from get_routing_llm to get_deepseek_llm** - `9ef6c42` (fix)

**Plan metadata:** (docs commit follows)

_Note: LLM fix was a post-Task-1 deviation applied as Rule 1 (bug fix — agents failing evals due to wrong LLM)_

## Files Created/Modified

- `tests/agents/test_full_pipeline.py` - 19 integration tests for full HTTP -> IntakeFlow -> specialist crew pipeline
- `tests/evals/run_evals.py` - CLI entry point for trajectory eval runs (dry-run works immediately)
- `src/agents/crews/auth_crew.py` - LLM switched to DeepSeek V3.2, docstring updated with eval rationale
- `src/agents/crews/municipal_crew.py` - LLM switched to DeepSeek V3.2, docstring updated
- `src/agents/crews/ticket_status_crew.py` - LLM switched to DeepSeek V3.2, docstring updated
- `src/agents/llm.py` - Module and function docstrings updated: get_deepseek_llm for all agents, get_routing_llm for IntakeFlow only
- `tests/agents/test_auth_crew.py` - Mock target updated from get_routing_llm to get_deepseek_llm

## Decisions Made

- DeepSeek V3.2 for all specialist agents: per-agent trajectory evals (previous session) proved gpt-4o-mini cannot follow long backstory prompts (150-300 lines). GBV crew (already on DeepSeek) was the only passing agent (1/4). Switching remaining 3 restores correct behavior.
- gpt-4o-mini retained ONLY for IntakeFlow intent classification: short prompt (<50 tokens), no tools, simple 5-way output — gpt-4o-mini reliable here.
- Live eval runners left as NotImplementedError: these require real API keys and are intended to be run manually. Dry-run mode is CI-safe.

## Deviations from Plan

### Auto-fixed Issues

**1. [Rule 1 - Bug] LLM switch: 3 crew files gpt-4o-mini -> DeepSeek V3.2**
- **Found during:** Pre-Task-2 (per-agent eval results from previous session showed 3/4 agents failing)
- **Issue:** gpt-4o-mini ignores backstory and tool usage instructions in long prompts (150-300 lines). Agents were not calling correct tools, not maintaining Gugu persona, not following registration/intake flows. Eval results: 0/3 tool calls correct for auth/municipal/ticket_status.
- **Fix:** Switched AuthCrew, MunicipalIntakeCrew, TicketStatusCrew from get_routing_llm() to get_deepseek_llm(). Updated docstrings in all 3 files. Updated llm.py module docstring. Updated test_auth_crew.py mock target.
- **Files modified:** src/agents/crews/auth_crew.py, src/agents/crews/municipal_crew.py, src/agents/crews/ticket_status_crew.py, src/agents/llm.py, tests/agents/test_auth_crew.py
- **Verification:** All 274 tests pass after change. Dry-run eval lists 25 scenarios correctly.
- **Committed in:** 9ef6c42

---

**Total deviations:** 1 auto-fixed (Rule 1 - critical bug: wrong LLM causing agent eval failure)
**Impact on plan:** The fix is a prerequisite for Plan 09 live evals. Without it, all 3 agents would fail the automated eval loop.

## Issues Encountered

- Phase 10.3 research doc recommended gpt-4o-mini for tool-heavy agents (Auth, Ticket Status). Empirical per-agent evals disproved this: gpt-4o-mini cannot follow 150-300 line system prompts that combine persona, tool instructions, and multi-step flows. Research assumption was based on gpt-4o-mini's general tool-use capability, not its behavior with long complex prompts.

## User Setup Required

None - no external service configuration required. Real LLM eval runs (live mode) require DEEPSEEK_API_KEY and OPENAI_API_KEY but these are already in .env.

## Next Phase Readiness

- Plan 09: Automated Playwright+Claude-judge dynamic eval loop — can now proceed with correct LLM assignment for all 4 agents
- Dry-run eval framework shows all 25 scenarios across 4 agents
- Live eval runners require filling in scenario runner bodies (NotImplementedError currently) — that is Plan 09's task
- Manual Streamlit smoke-test (Task 2) is the checkpoint before Plan 09

## Self-Check: PASSED

- tests/agents/test_full_pipeline.py: FOUND
- tests/evals/run_evals.py: FOUND
- src/agents/crews/auth_crew.py: FOUND (4 get_deepseek_llm references)
- src/agents/crews/municipal_crew.py: FOUND (4 get_deepseek_llm references)
- src/agents/crews/ticket_status_crew.py: FOUND (4 get_deepseek_llm references)
- src/agents/llm.py: FOUND
- tests/agents/test_auth_crew.py: FOUND
- Commit e58167b (Task 1): FOUND
- Commit 9ef6c42 (LLM fix): FOUND
- All 274 tests pass: VERIFIED

---
*Phase: 10.3-crewai-agent-rebuild-and-llm-evaluation-framework*
*Completed: 2026-02-26*
