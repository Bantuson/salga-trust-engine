---
phase: 06.9.1-fix-agent-output-formatting-pydantic-models-auth-otp-tool-failures-and-system-prompt-engineering
plan: 04
type: execute
wave: 3
depends_on: ["06.9.1-01", "06.9.1-02", "06.9.1-03"]
files_modified:
  - tests/test_output_formatting.py
  - tests/test_auth_tools.py
autonomous: true
requirements: [AI-01, AI-02, AI-03, AI-04, AI-05, AI-06, AI-07]

must_haves:
  truths:
    - "Unit tests verify sanitize_reply strips all known delegation and LLM artifact patterns"
    - "Unit tests verify Pydantic model validation and repair from adversarial raw output"
    - "Unit tests verify auth tool failure logging and repeated failure flagging"
    - "Integration tests verify end-to-end crew output is clean through the full sanitization pipeline"
    - "Mock adversarial outputs test graceful degradation without crashes"
    - "GBV error paths always include emergency numbers in test assertions"
  artifacts:
    - path: "tests/test_output_formatting.py"
      provides: "Unit + integration tests for sanitization, Pydantic validation, and delegation filtering"
      min_lines: 150
    - path: "tests/test_auth_tools.py"
      provides: "Unit tests for OTP tool fixes and failure logging"
      min_lines: 80
  key_links:
    - from: "tests/test_output_formatting.py"
      to: "src/api/v1/crew_server.py"
      via: "Tests import and call sanitize_reply, _validate_crew_output"
      pattern: "from src.api.v1.crew_server import"
    - from: "tests/test_output_formatting.py"
      to: "src/agents/crews/base_crew.py"
      via: "Tests import and call _repair_from_raw with model classes"
      pattern: "from src.agents.crews.base_crew import"
    - from: "tests/test_auth_tools.py"
      to: "src/agents/tools/auth_tool.py"
      via: "Tests verify _log_tool_failure and tool implementations with mocks"
      pattern: "from src.agents.tools.auth_tool import"
---

<objective>
Write comprehensive unit and integration tests for all Phase 6.9.1 changes: sanitize_reply delegation filtering, Pydantic model validation and repair, auth tool OTP fixes and failure logging, and end-to-end crew output pipeline.

Purpose: Per locked decision, tests include both unit tests (with mocks) and integration tests (with mocks), using canned realistic Gugu responses PLUS deliberately bad/adversarial outputs to test validation and sanitization. Pydantic validation is tested implicitly as part of integration flows.

Output: Two new test files with comprehensive coverage of all phase changes.
</objective>

<execution_context>
@C:/Users/Bantu/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Bantu/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06.9.1-fix-agent-output-formatting-pydantic-models-auth-otp-tool-failures-and-system-prompt-engineering/06.9.1-RESEARCH.md
@.planning/phases/06.9.1-fix-agent-output-formatting-pydantic-models-auth-otp-tool-failures-and-system-prompt-engineering/06.9.1-01-SUMMARY.md
@.planning/phases/06.9.1-fix-agent-output-formatting-pydantic-models-auth-otp-tool-failures-and-system-prompt-engineering/06.9.1-02-SUMMARY.md
@.planning/phases/06.9.1-fix-agent-output-formatting-pydantic-models-auth-otp-tool-failures-and-system-prompt-engineering/06.9.1-03-SUMMARY.md
@src/api/v1/crew_server.py
@src/agents/crews/base_crew.py
@src/agents/crews/municipal_crew.py
@src/agents/crews/gbv_crew.py
@src/agents/crews/ticket_status_crew.py
@src/agents/crews/auth_crew.py
@src/agents/crews/manager_crew.py
@src/agents/tools/auth_tool.py
@tests/test_manager_crew.py
@tests/test_municipal_crew.py
@tests/test_gbv_crew.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create test_output_formatting.py with unit and integration tests for sanitization and Pydantic validation</name>
  <files>
    tests/test_output_formatting.py
  </files>
  <action>
    Create a new test file `tests/test_output_formatting.py` with the following test sections. All tests use OPENAI_API_KEY=dummy at module level (or conftest). No real LLM calls.

    **Section 1: sanitize_reply unit tests (10+ tests)**

    Test cases for `from src.api.v1.crew_server import sanitize_reply`:

    1. `test_sanitize_clean_text_passthrough` — clean text "Hello! I'm Gugu..." passes through unchanged
    2. `test_sanitize_strips_final_answer_prefix` — "Final Answer: Hello!" becomes "Hello!"
    3. `test_sanitize_strips_thought_action_observation` — multi-line with "Thought: ...\nAction: ...\nObservation: ..." strips those lines, keeps citizen text
    4. `test_sanitize_strips_delegation_as_manager` — "As the Municipal Services Manager, here is the complete procedure for you, Gugu, to follow: Step 1..." is fully stripped. Must return fallback or citizen text after delegation.
    5. `test_sanitize_strips_delegation_routing_to` — "Routing to the auth specialist...\nHello! Let me help you register." strips first line, keeps second
    6. `test_sanitize_strips_i_am_delegating` — "I am delegating to the Crisis Support Specialist.\nI'm sorry to hear about your situation." strips delegation, keeps citizen text
    7. `test_sanitize_strips_step_numbering` — "Step 1: Send OTP to phone.\nStep 2: Verify OTP." is fully stripped (internal steps)
    8. `test_sanitize_strips_json_blobs` — embedded JSON `{"tracking_number": "TKT-...",...}` is removed
    9. `test_sanitize_empty_returns_fallback` — empty string returns warm Gugu fallback for given agent
    10. `test_sanitize_gbv_guarantees_emergency_numbers` — after sanitization of GBV response, "10111" and "0800 150 150" always present
    11. `test_sanitize_preserves_bold_formatting` — "Your tracking number is **TKT-20260209-ABC123**" preserves the bold
    12. `test_sanitize_fallback_per_language` — fallback in "zu" returns isiZulu Gugu fallback

    **Section 2: _validate_crew_output unit tests (5+ tests)**

    Test cases for `from src.api.v1.crew_server import _validate_crew_output`:

    1. `test_validate_clean_output_passthrough` — {"message": "Hello"} returns "Hello"
    2. `test_validate_delegation_polluted_extracts_citizen_text` — message starts with delegation but has citizen text below: extracts citizen text
    3. `test_validate_empty_message_returns_fallback` — {"message": ""} returns fallback
    4. `test_validate_short_message_returns_fallback` — {"message": "hi"} returns fallback (too short)
    5. `test_validate_missing_message_key_returns_fallback` — {} returns fallback

    **Section 3: Pydantic model validation and repair unit tests (8+ tests)**

    Test cases using `from src.agents.crews.base_crew import _repair_from_raw, AgentResponse`:
    Also import `MunicipalResponse`, `GBVResponse`, `TicketStatusResponse`.

    1. `test_repair_from_raw_json_extraction` — raw contains valid JSON: `'Some preamble {"message": "Hello", "language": "en"} some postamble'` -> extracts and validates
    2. `test_repair_from_raw_final_answer_extraction` — raw is "Final Answer: Your ticket has been created." -> extracts message
    3. `test_repair_from_raw_fallback_on_garbage` — raw is "asdf1234!@#$" -> returns fallback message
    4. `test_repair_never_crashes` — raw is None-like empty string -> returns fallback, no exception
    5. `test_municipal_response_strips_artifacts_in_message` — MunicipalResponse(message="Final Answer: Fixed the issue") -> message field is "Fixed the issue"
    6. `test_municipal_response_validates_language` — MunicipalResponse(message="test", language="xx") -> language defaults to "en"
    7. `test_gbv_response_requires_followup_default` — GBVResponse(message="test") -> requires_followup is True
    8. `test_ticket_status_response_tickets_found_default` — TicketStatusResponse(message="test") -> tickets_found is 0
    9. `test_auth_result_has_language_field` — AuthResult(authenticated=True, session_status="active", message="hi", language="zu") -> language is "zu"

    **Section 4: ManagerCrew parse_result delegation filtering (5+ tests)**

    Test cases using `from src.agents.crews.manager_crew import ManagerCrew, _DELEGATION_PATTERNS`:

    Create a FakeResult class that returns configurable string from __str__().

    1. `test_manager_parse_strips_full_delegation_block` — raw with "As the Municipal Services Manager, here is the complete procedure..." returns fallback
    2. `test_manager_parse_keeps_clean_citizen_text` — "Hello! I'm Gugu. How can I help?" passes through
    3. `test_manager_parse_strips_mixed_content` — delegation lines mixed with citizen text: strips delegation, keeps citizen text
    4. `test_manager_parse_extracts_tracking_number` — raw with TKT-20260219-ABC123 embedded: tracking_number in result
    5. `test_manager_parse_fallback_on_all_filtered` — raw that is entirely delegation text -> warm fallback message

    **Section 5: Integration test — full pipeline (3+ tests)**

    Mock-based integration tests that exercise the full path: crew parse_result -> _validate_crew_output -> sanitize_reply.

    1. `test_integration_municipal_clean_response` — canned MunicipalResponse dict through full pipeline: message survives intact
    2. `test_integration_adversarial_delegation_with_citizen_text` — adversarial output with delegation + citizen text: citizen text survives, delegation stripped
    3. `test_integration_gbv_error_always_has_emergency_numbers` — GBV crew error response through full pipeline: 10111 and 0800 150 150 always present

    **Mock data constants** at module level:
    - CLEAN_GUGU_RESPONSE_EN: "Hello! I'm Gugu from SALGA Trust Engine. How can I help you today?"
    - CLEAN_GUGU_RESPONSE_ZU: "Sawubona! NginguGugu we-SALGA Trust Engine. Ngingakusiza ngani namhlanje?"
    - ADVERSARIAL_DELEGATION_OUTPUT: "As the Municipal Services Manager, here is the complete and correct procedure for you, Gugu, to follow:\nStep 1: Ask the citizen for their name\nStep 2: Send OTP to their phone\nStep 3: Verify the code\nHello! Welcome to SALGA Trust Engine. What is your name?"
    - ADVERSARIAL_JSON_BLOB: '{"tracking_number": "TKT-20260219-ABC123", "status": "open", "id": "uuid-here"}\nYour report has been logged. Tracking number: TKT-20260219-ABC123.'
    - ADVERSARIAL_EMPTY_OUTPUT: ""
    - ADVERSARIAL_ONLY_ARTIFACTS: "Thought: I need to help this citizen\nAction: create_municipal_ticket\nAction Input: {}\nObservation: Error"

    All tests should be runnable with: `pytest tests/test_output_formatting.py -v`
  </action>
  <verify>
    ```
    OPENAI_API_KEY=dummy pytest tests/test_output_formatting.py -v --tb=short
    ```
    All tests pass. Count should be 30+.
  </verify>
  <done>
    30+ tests covering sanitize_reply, _validate_crew_output, Pydantic models, _repair_from_raw, ManagerCrew.parse_result delegation filtering, and full pipeline integration. Mock adversarial data exercises graceful degradation. GBV emergency number guarantee verified. All tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create test_auth_tools.py with OTP fix and failure logging tests</name>
  <files>
    tests/test_auth_tools.py
  </files>
  <action>
    Create a new test file `tests/test_auth_tools.py` with tests for auth tool fixes. All Supabase calls mocked with unittest.mock.patch.

    **Section 1: Tool failure logging tests (5+ tests)**

    Test `from src.agents.tools.auth_tool import _log_tool_failure, _tool_failure_counts`:

    1. `test_log_tool_failure_records_timestamp` — call once, verify _tool_failure_counts has 1 entry
    2. `test_log_tool_failure_prunes_old_entries` — manually inject timestamps > 5 min old, call again, verify old entries pruned
    3. `test_log_tool_failure_critical_on_three_failures` — call 3 times within 5 min, verify logger.critical is called (mock the logger)
    4. `test_log_tool_failure_error_on_single_failure` — call once, verify logger.error is called (not critical)
    5. `test_log_tool_failure_truncates_user_identifier` — verify long phone numbers are truncated to 8 chars + "..." for POPIA

    Clear _tool_failure_counts in setup/teardown to avoid state leakage between tests.

    **Section 2: send_otp_tool tests (5+ tests)**

    Mock `src.agents.tools.auth_tool.get_supabase_admin` to return a mock client.

    1. `test_send_otp_phone_uses_sms` — phone "+27821001001", channel="sms": mock verifies sign_in_with_otp called with {"phone": ...}
    2. `test_send_otp_email_uses_email` — email "test@test.com", channel="email": mock verifies sign_in_with_otp called with {"email": ...}
    3. `test_send_otp_returning_user_sets_should_create_user_false` — is_returning_user=True: mock verifies options.should_create_user is False
    4. `test_send_otp_new_user_no_should_create_user` — is_returning_user=False (default): mock verifies options does NOT include should_create_user
    5. `test_send_otp_failure_logs_error` — mock raises Exception: verify _log_tool_failure called, error string returned
    6. `test_send_otp_no_client_returns_error` — mock get_supabase_admin returns None: returns configuration error

    **Section 3: verify_otp_tool tests (4+ tests)**

    1. `test_verify_otp_phone_uses_type_sms` — phone "+27821001001": mock verifies type="sms"
    2. `test_verify_otp_email_uses_type_email` — email "test@test.com": mock verifies type="email" (NOT "magiclink")
    3. `test_verify_otp_success_returns_user_id` — mock returns user with id: verify "User ID: ..." in result
    4. `test_verify_otp_failure_logs_error` — mock raises Exception: verify _log_tool_failure called

    **Section 4: Existing tests regression check (1 test)**

    1. `test_all_tools_importable` — verify send_otp_tool, verify_otp_tool, create_supabase_user_tool, lookup_user_tool all importable and are callable

    All tests should be runnable with: `pytest tests/test_auth_tools.py -v`

    Run the FULL test suite at the end: `pytest tests/ -v --tb=short -x` to verify no regressions in existing tests.
  </action>
  <verify>
    ```
    OPENAI_API_KEY=dummy pytest tests/test_auth_tools.py -v --tb=short
    ```
    All tests pass. Count should be 15+.

    Regression check:
    ```
    OPENAI_API_KEY=dummy pytest tests/ -v --tb=short -x 2>&1 | tail -20
    ```
    No failures in existing tests.
  </verify>
  <done>
    15+ tests covering auth tool failure logging, OTP type correctness (email vs sms), should_create_user parameter, error handling, and tool importability. Full test suite passes with no regressions. _log_tool_failure critical flagging for 3+ failures verified.
  </done>
</task>

</tasks>

<verification>
1. `pytest tests/test_output_formatting.py -v` — 30+ tests pass
2. `pytest tests/test_auth_tools.py -v` — 15+ tests pass
3. `pytest tests/ -v --tb=short -x` — full suite passes, no regressions
4. Tests cover: sanitize_reply, _validate_crew_output, _repair_from_raw, all 4 Pydantic models, ManagerCrew.parse_result, auth tool OTP types, failure logging
5. Adversarial mock data exercises graceful degradation without crashes
6. GBV emergency number guarantee tested in error paths
</verification>

<success_criteria>
45+ new tests across 2 test files. All pass. No regressions in existing test suite. Adversarial/bad output scenarios tested with graceful degradation. GBV emergency numbers verified in all error paths. Auth OTP type correctness verified.
</success_criteria>

<output>
After completion, create `.planning/phases/06.9.1-fix-agent-output-formatting-pydantic-models-auth-otp-tool-failures-and-system-prompt-engineering/06.9.1-04-SUMMARY.md`
</output>
