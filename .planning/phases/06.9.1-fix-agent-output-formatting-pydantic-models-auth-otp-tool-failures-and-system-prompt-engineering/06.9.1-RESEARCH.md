# Phase 06.9.1: Fix Agent Output Formatting, Pydantic Models, Auth OTP Tool Failures, and System Prompt Engineering - Research

**Researched:** 2026-02-19
**Domain:** CrewAI agent quality, Pydantic structured output, Supabase OTP, system prompt engineering
**Confidence:** HIGH (codebase read directly; CrewAI and Supabase docs fetched from official sources)

---

<user_constraints>
## User Constraints (from CONTEXT.md)

### Locked Decisions

#### Response Formatting
- Light formatting for citizen messages — occasional bold for emphasis, numbered steps for instructions, but mostly conversational
- Adaptive response length — short (1-3 sentences) for confirmations, longer for explaining steps or gathering info
- Best-effort multi-layer precise stripping of LLM artifacts — graceful degradation, never crash
- Tracking numbers and reference IDs shown at key moments only (ticket creation, status check) — not repeated every message

#### Auth OTP Tool Failures
- Specific failures unknown — investigate from Streamlit logs and database logs as first research step
- Email OTP template was added to Supabase correctly but no emails have been getting through — investigate whether magic links or codes
- When OTP tool fails: Gugu apologizes, retries once, then gives manual alternative if still fails
- Log all tool failures with context (user_id, tool_name, error) AND flag repeated failures (3+ in 5 min) for urgent investigation

#### Pydantic Output Models
- Both layers: structured Pydantic returns from crews AND validation before sending to citizens
- All crews get Pydantic output models — ManagerCrew, AuthCrew, MunicipalCrew, GBVCrew, TicketStatusCrew (all are citizen-facing)
- Fields: message (clean text) + language (en/zu/af) + metadata (action_taken, requires_followup, etc.)
- On validation failure: attempt repair from raw output first, if repair fails then use safe Gugu-voiced fallback message (with emergency numbers for GBV)

#### System Prompt Engineering
- Internal reasoning and delegation leaked to citizens — this is the top critical issue
- Agent hallucinated "phone call verification" capability that doesn't exist
- Hard block in prompts: explicitly list available tools, say "do NOT mention any other methods"
- Agent YAML does not list tools — another best practice deviation to fix
- Per-crew strictness + universal guardrails: all crews get banned patterns, max length, required elements; GBV ultra-strict (trauma-informed), auth structured, municipal more flexible
- Internal agent-to-agent delegation hidden via code-level filtering in crew_server.py (strip delegation text before returning to citizen)
- Extensive, thorough CrewAI best practices research required — scrutinize every existing crew implementation against best practices, fix all deviations, no more regressions

#### Test Suite
- Both unit tests (with mocks) AND integration tests (with mocks) — full crew flows mocked end-to-end
- Mock data: canned realistic Gugu responses PLUS deliberately bad/adversarial outputs (artifacts, wrong language, missing fields) to test validation and sanitization
- Pydantic validation tested implicitly as part of integration flows (not standalone schema tests)
- Existing Phase 6.9 tests: Claude's discretion on which need updating vs leaving as regression baseline

### Claude's Discretion
- Exact Pydantic model field names and types for metadata
- Which existing 6.9 tests need updating vs preserving as-is
- Specific regex patterns for artifact stripping
- Mock response content for test scenarios
- Repair strategy implementation details

### Deferred Ideas (OUT OF SCOPE)
- None — discussion stayed within phase scope
</user_constraints>

---

<phase_requirements>
## Phase Requirements

| ID | Description | Research Support |
|----|-------------|-----------------|
| AI-01 | System uses CrewAI-based agentic architecture with manager agent receiving all incoming messages | ManagerCrew (Process.hierarchical) already in place; research confirms this is the correct pattern. Phase 6.9.1 adds Pydantic output models on top of existing architecture. |
| AI-02 | Manager agent analyzes message content and routes to appropriate specialist agent by category | Existing routing logic in manager_task YAML already covers 5 categories. Phase 6.9.1 adds code-level delegation text filtering in crew_server.py. |
| AI-03 | Specialist agent for municipal services handles report capture and ticket creation | MunicipalCrew already functional. Phase 6.9.1 adds Pydantic output model and prompt hardening. |
| AI-04 | Specialist agent for GBV/domestic violence/abuse handles sensitive report capture with enhanced privacy | GBVCrew already functional. Phase 6.9.1 adds ultra-strict system prompt guardrails and Pydantic model with emergency number guarantee. |
| AI-05 | Each specialist agent conducts structured conversational intake to gather required information | Phase 6.9.1 adds Pydantic-enforced structured output to ALL crews, plus prompt engineering to prevent hallucinated capabilities that disrupt intake flow. |
| AI-06 | Agents support English, isiZulu, and Afrikaans (detect language, respond in kind) | Language detection already working. Phase 6.9.1 adds `language` field to all Pydantic output models so language is explicitly tracked in every response. |
| AI-07 | All agent interactions have guardrails preventing inappropriate responses or data leakage | Phase 6.9.1 is the primary delivery for this requirement: Pydantic validation layer, enhanced sanitize_reply(), banned pattern lists in prompts, code-level delegation filtering, tool-name hard blocks in backstory/goal. |
</phase_requirements>

---

## Summary

This phase fixes four compounding quality failures that cause citizens to see internal agent behaviour. The critical transcript showing the manager agent leaking its entire delegation protocol to a citizen ("As the Municipal Services Manager, here is the complete and correct procedure for you, Gugu, to follow: Step 1...") is the highest severity issue. The root cause is a combination of: (1) system prompts that do not forbid internal narration, (2) no code-level filtering of delegation text before it reaches the citizen, (3) no Pydantic output validation that would force the model to produce a clean `message` field, and (4) the auth agent hallucinating capabilities (phone call verification) because its prompts do not explicitly enumerate permitted tools.

The OTP failure investigation reveals two distinct issues. First, the Supabase Magic Link email template must include `{{ .Token }}` to send a 6-digit code — if the template shows `{{ .ConfirmationURL }}` instead, users receive a magic link, not a code. This was partially addressed in phase 06.8 but emails are still not arriving, suggesting either an SMTP configuration problem or a template/OTP type mismatch on the verify side. Second, the `verify_otp` call uses `type: "sms"` for phone and should use `type: "email"` for email (the `type: "magiclink"` value is deprecated). The convo.md transcript shows the OTP flow at least appeared to trigger, so the agent is calling the tool — the failure is upstream in Supabase configuration.

The Pydantic output model strategy must add a second layer of structured output on top of the existing `sanitize_reply()` in crew_server.py. The existing AuthCrew already uses `output_pydantic=AuthResult` on its Task. The other crews (MunicipalCrew, GBVCrew, TicketStatusCrew, and the response shape from ManagerCrew) need equivalent models. The repair strategy for validation failures is: (1) attempt to parse `result.raw` or `str(result)` with JSON extraction regex, (2) fill a fallback model with the extracted message field, (3) if that also fails, return a hardcoded safe fallback that is warm, Gugu-voiced, and for GBV always includes emergency numbers.

**Primary recommendation:** Fix system prompts first (highest citizen impact), then add Pydantic models to remaining crews, then fix OTP verification, then add code-level delegation filtering in crew_server.py, then write the test suite.

---

## Standard Stack

### Core (already installed, no new dependencies needed)

| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| crewai | Installed | Multi-agent orchestration, Task output_pydantic | Framework in use; output_pydantic is the official structured output mechanism |
| pydantic | v2 | Structured output models, validation, repair | CrewAI uses Pydantic v2 internally; BaseModel is the standard |
| python-supabase | Installed | Auth OTP send/verify | Already in use for auth_tool.py |
| re (stdlib) | stdlib | Regex for artifact stripping, repair | No additional dependency needed |
| json (stdlib) | stdlib | JSON extraction for repair strategy | Already used in base_crew.py |

### Supporting

| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| pydantic.model_validator | v2 | Cross-field validation, repair on model | When a field depends on another field or needs post-init cleanup |
| pydantic.field_validator | v2 | Per-field sanitization | When a specific field needs cleaning (strip artifacts from `message`) |

### No New Dependencies

This phase adds NO new pip packages. All tooling is already present. The Pydantic models, JSON repair, and regex filtering use only what is already installed.

---

## Architecture Patterns

### Recommended File Structure (changes only)

```
src/agents/
├── crews/
│   ├── base_crew.py          # Add: AgentResponse base Pydantic model + repair logic
│   ├── auth_crew.py          # Update: AuthResult model fields, no structural change
│   ├── municipal_crew.py     # Add: MunicipalResponse Pydantic model + output_pydantic
│   ├── gbv_crew.py           # Add: GBVResponse Pydantic model + output_pydantic
│   ├── ticket_status_crew.py # Add: TicketStatusResponse Pydantic model + output_pydantic
│   └── manager_crew.py       # Add: code-level delegation text filter in parse_result()
├── config/
│   ├── agents.yaml           # Update: add tools list comments (docs only, tools via code)
│   └── tasks.yaml            # Update: add per-crew guardrail instructions to expected_output
├── prompts/
│   ├── auth.py               # Update: AUTH_PROMPT_EN/ZU/AF — hard-block tool enumeration
│   ├── municipal.py          # Update: hard-block non-existent methods
│   └── gbv.py                # Update: ultra-strict guardrails
└── tools/
    └── auth_tool.py          # Update: fix verify_otp type="email", add tool failure logging
src/api/v1/
└── crew_server.py            # Update: delegation text filter, enhanced sanitize_reply()
tests/
├── test_output_formatting.py # New: unit + integration tests for sanitization and Pydantic models
└── test_auth_crew.py         # New: unit tests for OTP tool failure handling + retry logic
```

### Pattern 1: Pydantic Output Model on Task (CrewAI official pattern)

**What:** Pass a Pydantic BaseModel class as `output_pydantic` to `Task()`. CrewAI converts the LLM output into the model, accessible via `result.pydantic`.

**When to use:** On every specialist crew task that produces a citizen-facing response.

**Confirmed from:** Official CrewAI docs at https://docs.crewai.com/en/concepts/tasks

```python
# Source: https://docs.crewai.com/en/concepts/tasks (confirmed 2026-02-19)
from pydantic import BaseModel
from crewai import Task

class MunicipalResponse(BaseModel):
    message: str           # Clean citizen-facing text
    language: str          # "en" | "zu" | "af"
    action_taken: str      # "intake_started" | "ticket_created" | "clarifying"
    requires_followup: bool
    tracking_number: str | None = None
    raw_output: str | None = None  # For debug — not citizen-facing

task = Task(
    description=task_description,
    expected_output="A MunicipalResponse JSON with message, language, action_taken, requires_followup",
    agent=agent,
    output_pydantic=MunicipalResponse,  # KEY: structured output
)
```

**Accessing the result:**
```python
result = crew.kickoff(inputs=inputs)
if hasattr(result, "pydantic") and result.pydantic:
    model: MunicipalResponse = result.pydantic
    return model.model_dump()
# Fallback: result.raw contains the raw string output
```

### Pattern 2: Pydantic Repair from Raw Output

**What:** When `result.pydantic` is None (Pydantic conversion failed), attempt to extract a valid model from `result.raw` using JSON regex extraction.

**When to use:** In `parse_result()` of every crew. This is the repair strategy for validation failures.

**Rationale:** CrewAI logs "Failed to convert text into a pydantic model... Using raw output instead" when conversion fails. The raw output often contains a valid JSON object embedded in prose text.

```python
# Repair strategy (Claude's discretion on implementation)
import json, re

def _repair_from_raw(raw: str, model_class, fallback_message: str) -> dict:
    """Attempt to extract model fields from raw LLM output string.

    Strategy:
    1. Try to find a JSON object in the raw string
    2. Validate extracted dict against model_class
    3. If both fail, return a hardcoded safe fallback
    """
    # Step 1: Extract JSON block if present
    json_match = re.search(r'\{[^{}]*"message"[^{}]*\}', raw, re.DOTALL)
    if json_match:
        try:
            data = json.loads(json_match.group())
            return model_class(**data).model_dump()
        except Exception:
            pass

    # Step 2: Extract text after "Final Answer:" marker
    final_match = re.search(r"Final Answer:?\s*(.+)", raw, re.DOTALL | re.IGNORECASE)
    extracted_msg = final_match.group(1).strip() if final_match else raw.strip()

    # Step 3: Build minimal valid model from extracted text
    try:
        return model_class(message=extracted_msg or fallback_message).model_dump()
    except Exception:
        # Final fallback: hardcoded safe message (never fails)
        return {"message": fallback_message, "language": "en",
                "action_taken": "error", "requires_followup": False}
```

### Pattern 3: System Prompt Tool Hard-Block

**What:** In agent backstory and goal, explicitly list ONLY the tools available, followed by a hard prohibition on mentioning any other method.

**When to use:** All agent prompts — especially auth agent which hallucinated phone call verification.

**Rationale confirmed from:** https://community.crewai.com/t/hallucination-is-happening-by-agents/7037 and CrewAI blog: agents hallucinate less when bounded to explicit tool names.

```python
# Source: Derived from CrewAI community best practices + this codebase's failure transcript
AUTH_PROMPT_EN_HARDENED = """...existing content...

AVAILABLE TOOLS (COMPLETE LIST — USE ONLY THESE):
1. send_otp_tool — sends a 6-digit verification code to phone (channel="sms") or email (channel="email")
2. verify_otp_tool — verifies the code the citizen provides
3. create_supabase_user_tool — creates the citizen's account after successful verification
4. lookup_user_tool — checks whether a citizen already has an account

DO NOT mention, suggest, or offer any other verification method. You cannot:
- Verify via phone call (no such tool exists)
- Verify via WhatsApp message (no such tool exists)
- Verify via video call (no such tool exists)
- Skip verification and proceed anyway

If a tool fails: apologize clearly, tell the citizen what happened, and retry ONCE.
If it fails again: tell the citizen you are unable to complete that step right now and give them a manual alternative contact.

NEVER narrate your internal reasoning steps to the citizen. NEVER say:
- "Step 1: I need to..."
- "As the auth specialist, I will..."
- "I am delegating to..."
- "The manager has assigned me to..."
"""
```

### Pattern 4: Code-Level Delegation Text Filtering

**What:** In `crew_server.py`'s `sanitize_reply()` and in `ManagerCrew.parse_result()`, strip lines that look like internal delegation text before they reach the citizen.

**When to use:** As a defense layer after Pydantic validation and prompt hardening — belt-and-suspenders.

**Rationale:** The critical transcript showed the manager leaking "As the Municipal Services Manager, here is the complete and correct procedure for you, Gugu, to follow:" — this is delegation narration. Even with improved prompts, this must be filtered at the code level.

```python
# Additional patterns for _LLM_ARTIFACT_PATTERNS in crew_server.py sanitize_reply()
_DELEGATION_ARTIFACT_PATTERNS = [
    r"^As the .* Manager.*$",                          # Role narration
    r"^As the .* Specialist.*$",                       # Role narration
    r"^Here is the complete.*procedure.*$",            # Delegation instructions
    r"^Step \d+:.*",                                   # Numbered internal steps
    r"^The manager has.*$",                            # Manager reference
    r"^I am delegating.*$",                            # Delegation narration
    r"^You, Gugu.*$",                                  # Misidentified citizen as agent
    r"^Dear Gugu.*$",                                  # Same
    r"^Procedure for.*specialist.*$",                  # Internal routing
    r"^Routing to.*$",                                 # Internal routing
    r"^Delegating to.*$",                              # Internal routing
]
```

### Pattern 5: Tool Failure Retry + Structured Logging

**What:** Wrap tool calls in auth_tool.py with structured failure logging and a retry count mechanism. Surface this to the agent via the tool return string.

**When to use:** For all auth tools (send_otp_tool, verify_otp_tool), which are the reported failure points.

```python
# Tool failure logging pattern for auth_tool.py
import logging
import time
from collections import defaultdict

_tool_failure_counts: dict = defaultdict(list)

def _log_tool_failure(tool_name: str, error: str, user_identifier: str) -> None:
    """Log tool failure. Flag if 3+ failures in 5 minutes (locked decision)."""
    logger = logging.getLogger("auth_tools")
    now = time.time()
    # Prune failures older than 5 minutes
    _tool_failure_counts[tool_name] = [
        t for t in _tool_failure_counts[tool_name] if now - t < 300
    ]
    _tool_failure_counts[tool_name].append(now)

    count = len(_tool_failure_counts[tool_name])
    log_data = {
        "tool": tool_name, "error": error,
        "user": user_identifier[:8] + "...",  # Partial ID only (POPIA)
        "failure_count_5min": count,
    }
    if count >= 3:
        logger.critical("URGENT: Repeated tool failures — %s", log_data)
    else:
        logger.error("Tool failure — %s", log_data)
```

### Anti-Patterns to Avoid

- **Pydantic on ManagerCrew Task directly:** ManagerCrew uses `Process.hierarchical` with a single manager task — `output_pydantic` on the manager task causes issues because the manager's output is the final assembled result, not a specialist result. Apply Pydantic models to SPECIALIST crew tasks only. ManagerCrew.parse_result() does code-level cleanup and returns its own dict structure.
- **`tools=[]` in agents.yaml:** The agents.yaml `tools` key is not read by CrewAI's YAML loader for programmatic crew construction — tools must be passed at `Agent()` instantiation. Do NOT add tool instances to YAML (YAML is config text, not Python objects). The context decision that "agent YAML does not list tools" means document them in YAML comments, not as functional YAML keys.
- **`type: "magiclink"` in verify_otp:** Deprecated. Use `type: "email"` for email OTP verification. This is confirmed by Supabase official docs.
- **`should_create_user` not set to False for re-auth:** When `send_otp_tool` is called for an existing user re-authentication, if `should_create_user` defaults to True, a new account may be silently created instead of sending OTP to the existing account. For returning users, set `options.should_create_user=False`.
- **`expected_output` omitted when using `output_pydantic`:** CrewAI requires `expected_output` as a string even when `output_pydantic` is set. Omitting it causes a validation error. Both must be present.
- **Trusting `result.pydantic` always succeeds:** With DeepSeek LLM, `output_pydantic` conversion may fail silently — CrewAI logs it and falls back to `result.raw`. Always check `hasattr(result, 'pydantic') and result.pydantic` before using it, then fall back to `result.raw` for the repair strategy.

---

## Existing Codebase State: What Already Works vs What Needs Fixing

### What Already Works (DO NOT REGRESS)

| Component | Status | Evidence |
|-----------|--------|----------|
| `sanitize_reply()` in crew_server.py | Working — Phase 06.8 | Lines 348-414 in crew_server.py |
| `AuthResult` Pydantic model + `output_pydantic=AuthResult` on auth task | Working | auth_crew.py lines 26-43, 31 |
| `parse_result()` for auth crew (Pydantic + fallback paths) | Working | auth_crew.py lines 43-63 |
| `BaseCrew` abstract base with `parse_result()`, `get_error_response()` | Working | base_crew.py |
| Manager agent `tools=[]` (CRITICAL, must stay empty) | Correct | manager_crew.py line 92 |
| `Process.hierarchical` + `manager_agent` + `manager_llm` dual set | Correct | manager_crew.py lines 191-196 |
| Short-circuit specialist routing via `routing_phase` | Working | crew_server.py lines 723-742 |
| Defense-in-depth: crews strip Final Answer, crew_server.py sanitizes | Partial | Both layers exist but patterns are incomplete |
| `memory=False` on all crews | Correct | All crew classes |
| `allow_delegation=False` on all specialists | Correct | Verified in test_manager_crew.py |

### What Needs Fixing (This Phase)

| Component | Problem | Fix Location |
|-----------|---------|--------------|
| Manager agent outputs internal delegation text to citizen | Missing delegation text filter; prompts allow narration | crew_server.py `sanitize_reply()` + `manager_crew.py` `parse_result()` |
| Agent hallucinated "phone call verification" | Prompts do not enumerate tools or prohibit non-existent methods | `src/agents/prompts/auth.py` AUTH_PROMPT_EN/ZU/AF |
| `verify_otp_tool` uses `type: "sms"` for all | Should use `type: "email"` for email path | `src/agents/tools/auth_tool.py` |
| Email OTP sending: `should_create_user` not set | Re-auth sends to new account risk | `src/agents/tools/auth_tool.py` `_send_otp_impl` |
| No tool failure logging | Silent failures; no urgency flagging | `src/agents/tools/auth_tool.py` |
| MunicipalCrew has no Pydantic output model | parse_result() only does regex | `src/agents/crews/municipal_crew.py` + `src/agents/prompts/municipal.py` |
| GBVCrew has no Pydantic output model | parse_result() only calls super() | `src/agents/crews/gbv_crew.py` |
| TicketStatusCrew has no Pydantic output model | parse_result() only adds agent key | `src/agents/crews/ticket_status_crew.py` |
| `sanitize_reply()` missing delegation artifact patterns | Current patterns don't catch "As the Manager..." text | `src/api/v1/crew_server.py` |
| Manager task YAML instructs delegation via role names but does not forbid narration | Manager describes delegation steps to citizen | `src/agents/config/tasks.yaml` manager_task section |
| No test coverage for sanitization with adversarial inputs | No tests exist for bad/adversarial agent output | New test file needed |

---

## Don't Hand-Roll

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| JSON repair from messy LLM output | Custom parser from scratch | `re.search()` JSON extraction + `model_class(**data)` | Edge cases in JSON repair are numerous; minimal regex + Pydantic's own validation handles 95% of cases |
| Output validation beyond type checking | Custom validation framework | Pydantic `field_validator` + `model_validator` | Pydantic v2 already does this; re-implementing adds maintenance debt |
| Tool retry logic | Async retry framework | Simple `try / retry once / fallback` in the tool itself | CrewAI tools are synchronous; complex retry frameworks add import overhead |
| Language detection inside Pydantic model | Custom language scorer | Already done in `crew_server.py` (`language_detector.detect()`); just pass detected language into model |  |
| Delegation text filtering NLP | Sentence classifier | Simple regex patterns; delegation text is highly formulaic | Over-engineering for a pattern recognition problem that regex handles cleanly |

**Key insight:** All the hard infrastructure (Pydantic, CrewAI, language detection, sanitization) is already in place. This phase is primarily prompt engineering and adding Pydantic to the 4 crews that lack it.

---

## Common Pitfalls

### Pitfall 1: ManagerCrew output_pydantic vs parse_result
**What goes wrong:** Adding `output_pydantic=ManagerResponse` to the manager task in ManagerCrew causes CrewAI hierarchical mode to fail — the manager task result aggregates specialist outputs and its format is not a single Pydantic model.
**Why it happens:** In hierarchical mode, the final result is assembled from multiple specialist outputs; forcing it into a Pydantic schema breaks the aggregation.
**How to avoid:** Do NOT add `output_pydantic` to the manager task. Apply it only to specialist crew tasks (AuthCrew, MunicipalCrew, GBVCrew, TicketStatusCrew). ManagerCrew's `parse_result()` handles cleanup with regex + delegation filtering.
**Warning signs:** `ValidationError` or `AttributeError` on `result.pydantic` after adding output_pydantic to manager task.

### Pitfall 2: Supabase verify_otp type mismatch for email
**What goes wrong:** Calling `verify_otp({"email": addr, "token": code, "type": "magiclink"})` fails or silently rejects valid OTP codes because `magiclink` type is deprecated.
**Why it happens:** The current `_verify_otp_impl` in auth_tool.py uses `"type": "sms"` for phone and was written before the Supabase deprecation of "magiclink".
**How to avoid:** For email verification: `type: "email"`. For phone verification: `type: "sms"`. These are the only two valid non-deprecated types for citizen sign-in.
**Source:** https://supabase.com/docs/reference/python/auth-verifyotp (confirmed 2026-02-19)

```python
# CORRECT email verify_otp call
result = client.auth.verify_otp({
    "email": phone_or_email,
    "token": otp_code,
    "type": "email",  # NOT "magiclink" (deprecated)
})
```

### Pitfall 3: Supabase email template — Magic Link vs OTP code
**What goes wrong:** Email is sent but citizen receives a clickable link, not a 6-digit code. The agent then asks for a code that the citizen cannot provide.
**Why it happens:** The Supabase "Magic Link" email template defaults to `{{ .ConfirmationURL }}`. If the template was not changed to `{{ .Token }}`, the OTP path silently sends a link instead.
**How to avoid:** The Supabase dashboard Magic Link template MUST contain `{{ .Token }}` in the body. The otp-supabase-paste.html template file already uses `{{ .Token }}` and has been pasted into Supabase. If emails are not arriving at all, the SMTP configuration must be checked separately.
**Warning signs:** Citizen says they received an email but it has a "Click here" link rather than a number; or citizen says no email was received at all (SMTP issue).

### Pitfall 4: Pydantic validation fails silently, raw string returned
**What goes wrong:** `result.pydantic` is None even though `output_pydantic` is set on the task. The crew falls through to the raw string path, losing all structured fields.
**Why it happens:** DeepSeek LLM does not always produce valid JSON conforming to the model schema. CrewAI logs "Failed to convert... Using raw output instead" but does not raise an exception.
**How to avoid:** Always check `hasattr(result, "pydantic") and result.pydantic is not None`. Use `result.raw` as input to the repair function when it is None.
**Warning signs:** `result.pydantic` is None in parse_result(); all fields default to fallback.

### Pitfall 5: expected_output required even when output_pydantic is set
**What goes wrong:** `Task()` raises `ValidationError: expected_output Field required` when `expected_output` is omitted and only `output_pydantic` is provided.
**Why it happens:** CrewAI requires both fields — `output_pydantic` for the schema, `expected_output` as a human-readable description used in the task's internal prompt.
**How to avoid:** Always include both:
```python
task = Task(
    description=...,
    expected_output="A JSON response with message (str), language (str), action_taken (str), requires_followup (bool)",  # Required
    agent=agent,
    output_pydantic=MunicipalResponse,  # Also required for structured output
)
```
**Warning signs:** `ValidationError` on Task construction mentioning `expected_output`.

### Pitfall 6: Auth agent "phone call verification" hallucination
**What goes wrong:** The auth agent offers to verify the citizen via phone call, which is a capability that does not exist.
**Why it happens:** The current prompts only tell the agent WHAT to do (send OTP, verify OTP) but do not explicitly prohibit other methods. The LLM fills in the gap with plausible-sounding but non-existent capabilities.
**How to avoid:** Add an explicit "AVAILABLE TOOLS (COMPLETE LIST)" section followed by "DO NOT suggest or offer any other verification method." The prohibition must be explicit, not implied.
**Warning signs:** Agent response contains words like "call", "phone call", "video", or verification methods not in the tools list.

### Pitfall 7: Delegation text reaches citizen despite prompt changes
**What goes wrong:** Even after hardening prompts, the manager agent occasionally leaks delegation text when the LLM chain-of-thought is verbose.
**Why it happens:** Prompt instructions are soft constraints on LLMs. Under certain inputs (long conversations, ambiguous intents), the model reverts to narrating its reasoning.
**How to avoid:** Belt-and-suspenders: prompt hardening PLUS code-level filtering in `sanitize_reply()` PLUS `ManagerCrew.parse_result()` delegation pattern stripping. All three layers must be present.
**Warning signs:** Citizen receives a reply containing "As the Municipal Services Manager" or "Step 1: I need to".

---

## Code Examples

Verified patterns from codebase inspection and official docs:

### Pydantic Model for Municipal Crew (recommended shape)

```python
# Source: Derived from existing AuthResult pattern in auth.py + CrewAI docs
from pydantic import BaseModel, field_validator

class MunicipalResponse(BaseModel):
    """Structured output from MunicipalCrew."""
    message: str                    # Clean citizen-facing text
    language: str = "en"           # "en" | "zu" | "af"
    action_taken: str = "intake"   # "intake_started" | "ticket_created" | "clarifying"
    requires_followup: bool = False
    tracking_number: str | None = None

    @field_validator("message")
    @classmethod
    def strip_artifacts(cls, v: str) -> str:
        """Strip LLM artifacts from message field if present."""
        import re
        final_match = re.search(r"Final Answer:?\s*(.+)", v, re.DOTALL | re.IGNORECASE)
        if final_match:
            return final_match.group(1).strip()
        return v.strip()

    @field_validator("language")
    @classmethod
    def validate_language(cls, v: str) -> str:
        return v if v in ("en", "zu", "af") else "en"
```

### GBV Response Model (must guarantee emergency numbers)

```python
# Source: Derived from locked decisions + GBVCrew.get_error_response()
class GBVResponse(BaseModel):
    """Structured output from GBVCrew. Emergency numbers always in error fallback."""
    message: str
    language: str = "en"
    action_taken: str = "safety_check"  # "safety_check" | "report_filed" | "escalated"
    requires_followup: bool = True       # GBV always requires followup
    tracking_number: str | None = None

    # Validation: message must include emergency numbers if action is error
    @field_validator("message")
    @classmethod
    def ensure_safety_info_present(cls, v: str) -> str:
        """Strip artifacts from message."""
        import re
        final_match = re.search(r"Final Answer:?\s*(.+)", v, re.DOTALL | re.IGNORECASE)
        if final_match:
            return final_match.group(1).strip()
        return v.strip()
```

### Correct Supabase OTP send for existing users (re-auth path)

```python
# Source: https://supabase.com/docs/reference/python/auth-signinwithotp (confirmed 2026-02-19)
# For returning users — do NOT create new account
client.auth.sign_in_with_otp({
    "email": email_address,
    "options": {
        "should_create_user": False,  # CRITICAL for re-auth: don't create duplicate
    }
})

# For new users — allow account creation (default)
client.auth.sign_in_with_otp({
    "email": email_address,
    # should_create_user defaults to True — correct for new user path
})
```

### Correct Supabase OTP verify for email

```python
# Source: https://supabase.com/docs/reference/python/auth-verifyotp (confirmed 2026-02-19)
# type="email" is correct; type="magiclink" is DEPRECATED

# Phone verification
result = client.auth.verify_otp({
    "phone": phone_number,
    "token": otp_code,
    "type": "sms",
})

# Email verification
result = client.auth.verify_otp({
    "email": email_address,
    "token": otp_code,
    "type": "email",      # NOT "magiclink" (deprecated since ~2024)
})
```

### Delegation text filter additions for sanitize_reply()

```python
# Source: Derived from critical failure transcript in CONTEXT.md specifics section
# Add to _LLM_ARTIFACT_PATTERNS in crew_server.py

_DELEGATION_ARTIFACT_PATTERNS = [
    r"^As the .{0,50} Manager.*$",
    r"^As the .{0,50} Specialist.*$",
    r"^Here is the complete.*procedure.*$",
    r"^Step \d+[:\.].*",              # Numbered internal steps (NOT citizen steps)
    r"^For you, Gugu.*$",             # Misidentified agent as citizen
    r"^Dear Gugu.*$",
    r"^I am now delegating.*$",
    r"^Routing to.*specialist.*$",
    r"^Delegating to.*$",
    r"^Procedure for Gugu.*$",
]
```

### Hardened auth prompt pattern (tool enumeration + hard block)

```python
# Source: Derived from CrewAI community best practices + failure transcript analysis
# Add to bottom of AUTH_PROMPT_EN in src/agents/prompts/auth.py

_AUTH_TOOL_HARD_BLOCK = """
AVAILABLE TOOLS — COMPLETE AND FINAL LIST:
You have access to EXACTLY these tools:
1. send_otp_tool(phone_or_email, channel) — sends 6-digit code via SMS or email
2. verify_otp_tool(phone_or_email, otp_code, otp_type) — verifies the code
3. create_supabase_user_tool(phone_or_email, full_name, tenant_id) — creates account
4. lookup_user_tool(phone_or_email) — checks if citizen has an existing account

YOU DO NOT HAVE AND CANNOT OFFER:
- Phone call verification (DOES NOT EXIST)
- WhatsApp code delivery (NOT AVAILABLE)
- Video verification (DOES NOT EXIST)
- Any method not listed above

RESPONSE RULES — MANDATORY:
- NEVER narrate your reasoning steps to the citizen
- NEVER say "Step 1:", "Step 2:" in a citizen-facing message
- NEVER describe your role or who assigned you
- NEVER say "As the authentication specialist..."
- NEVER say "The manager has asked me to..."
- Speak DIRECTLY to the citizen as if you are a friendly human helper
- If a tool fails: "I'm sorry, there was a technical issue sending your code. Let me try once more."
"""
```

---

## State of the Art (CrewAI Best Practices)

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| Tools listed in agents.yaml | Tools passed via code only to Agent() | CrewAI design; YAML handles role/goal/backstory only | YAML cannot hold Python tool objects; code-only is correct |
| `type: "magiclink"` for email OTP verify | `type: "email"` | Supabase deprecated magiclink type ~2024 | Using old type causes silent failures or rejection |
| Generic `expected_output` when using output_pydantic | Both fields required: `expected_output` (string) + `output_pydantic` (class) | CrewAI requirement (all versions) | Missing either causes ValidationError |
| Trust LLM to never narrate reasoning | Hard block in prompt + code-level filter | CrewAI community experience 2025 | Prompt alone insufficient; code layer required |
| Single sanitization layer | Defence-in-depth: crew strip + crew_server.py sanitize | Phase 06.8 + 06.9.1 | Layered defence handles LLM unpredictability |

**Deprecated/outdated in this codebase:**
- `type: "sms"` for email verify_otp calls — the current code uses `"sms"` as the type for the email path. This must be changed to `"email"`.
- verbose=True in ManagerCrew (line 198 in manager_crew.py) — this was enabled for testing but increases chances of verbose output leaking into responses. Should be set to False in production.

---

## Open Questions

1. **Why are email OTPs not arriving despite template being set correctly?**
   - What we know: The otp-supabase-paste.html template uses `{{ .Token }}` correctly. The convo.md shows the agent claimed to send the OTP. The debug log (auth-agent-lang-pref-otp.md) confirms the template was pasted into Supabase.
   - What's unclear: Is the custom SMTP configured in Supabase, or is it using the built-in email service? Built-in Supabase email service has rate limits and deliverability issues in production. Also unclear: whether the OTP is being sent to the "Magic Link" template vs the "Confirm signup" template.
   - Recommendation: During Plan execution, check Supabase Auth logs for email delivery status. If SMTP is not configured, add a note in auth_tool.py that Supabase's built-in SMTP is rate-limited and unreliable for production.

2. **Should `verbose=True` in ManagerCrew remain for production?**
   - What we know: ManagerCrew.create_crew() has `verbose=True` (line 198). This was noted as "Enabled for testing."
   - What's unclear: Whether verbose output from CrewAI goes to stdout only or can leak into result strings.
   - Recommendation: Set `verbose=False` in production mode. CrewAI verbose output goes to stdout/logs, not into `result.raw`, so the risk is low but it's still best practice.

3. **Pydantic repair on non-JSON raw output — how deep should the repair go?**
   - What we know: The locked decision says "attempt repair from raw output first, if repair fails use safe fallback." Specific implementation is Claude's discretion.
   - What's unclear: Whether to attempt multi-step repair (JSON extraction → Final Answer extraction → partial field extraction) or stop after one attempt.
   - Recommendation: Two-step repair only: (1) JSON regex extraction, (2) Final Answer extraction as message field. Three-step gets too complex and increases the risk of passing malformed data into the model.

---

## Sources

### Primary (HIGH confidence)
- `src/agents/crews/auth_crew.py` — existing AuthResult + parse_result pattern (read directly)
- `src/agents/crews/base_crew.py` — BaseCrew architecture (read directly)
- `src/agents/crews/manager_crew.py` — ManagerCrew Process.hierarchical implementation (read directly)
- `src/api/v1/crew_server.py` — existing sanitize_reply() + delegation short-circuit (read directly)
- `src/agents/tools/auth_tool.py` — send_otp_tool and verify_otp_tool implementations (read directly)
- `src/agents/config/agents.yaml` — all agent role/goal/backstory configs (read directly)
- `src/agents/config/tasks.yaml` — all task description/expected_output configs (read directly)
- `src/agents/prompts/auth.py` — AUTH_PROMPT_EN/ZU/AF (read directly)
- `.planning/debug/resolved/auth-agent-lang-pref-otp.md` — confirmed root causes for OTP issues (read directly)
- `convo.md` — actual failure transcript showing manager leaking delegation text (read directly)
- https://docs.crewai.com/en/concepts/tasks — output_pydantic usage, expected_output required, result.pydantic access (fetched 2026-02-19)
- https://docs.crewai.com/en/concepts/agents — YAML vs code tool configuration, system_template (fetched 2026-02-19)
- https://supabase.com/docs/reference/python/auth-verifyotp — type="email" for email OTP, type="magiclink" deprecated (fetched 2026-02-19)
- https://supabase.com/docs/reference/python/auth-signinwithotp — should_create_user parameter (fetched 2026-02-19)
- https://supabase.com/docs/guides/auth/auth-email-templates — Magic Link vs OTP template variables, {{ .Token }} vs {{ .ConfirmationURL }} (fetched 2026-02-19)

### Secondary (MEDIUM confidence)
- https://community.crewai.com/t/hallucination-is-happening-by-agents/7037 — community confirmation that explicit tool lists reduce hallucination
- https://community.crewai.com/t/issue-with-crewai-agent-output-format-validation/7273 — Pydantic conversion failures with DeepSeek; raw fallback behaviour
- https://supabase.com/docs/guides/auth/auth-email-passwordless — OTP vs magic link distinction (confirmed by official docs)

### Tertiary (LOW confidence)
- None — all critical findings were verified against official docs or direct codebase inspection.

---

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH — all libraries already in use; no new dependencies
- Architecture patterns: HIGH — Pydantic output_pydantic verified from CrewAI official docs; repair strategy is Claude's discretion (MEDIUM)
- OTP fix: HIGH for verify_otp type parameter (official docs confirmed); MEDIUM for "why emails not arriving" (requires Streamlit/Supabase log investigation at plan time)
- Prompt hardening patterns: HIGH — failure transcript is direct evidence; patterns derived from actual failure
- Pitfalls: HIGH — most derived from codebase bugs already observed in production testing

**Research date:** 2026-02-19
**Valid until:** 2026-03-19 for stable findings (Pydantic, architecture); 2026-02-26 for Supabase OTP config (fast-moving)
