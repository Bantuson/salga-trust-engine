# Phase 6.9.1: Fix agent output formatting, Pydantic models, auth OTP tool failures, and system prompt engineering - Context

**Gathered:** 2026-02-19
**Status:** Ready for planning

<domain>
## Phase Boundary

Fix agent behavior quality across all crews: eliminate internal reasoning leakage to citizens, add Pydantic structured output models for all crews, fix auth OTP tool execution failures, and re-engineer system prompts against CrewAI best practices. Audit every existing CrewAI implementation for best practice deviations — no more regressions.

</domain>

<decisions>
## Implementation Decisions

### Response formatting
- Light formatting for citizen messages — occasional bold for emphasis, numbered steps for instructions, but mostly conversational
- Adaptive response length — short (1-3 sentences) for confirmations, longer for explaining steps or gathering info
- Best-effort multi-layer precise stripping of LLM artifacts — graceful degradation, never crash
- Tracking numbers and reference IDs shown at key moments only (ticket creation, status check) — not repeated every message

### Auth OTP tool failures
- Specific failures unknown — **investigate from Streamlit logs and database logs** as first research step
- Email OTP template was added to Supabase correctly but no emails have been getting through — investigate whether magic links or codes
- When OTP tool fails: Gugu apologizes, retries once, then gives manual alternative if still fails
- Log all tool failures with context (user_id, tool_name, error) AND flag repeated failures (3+ in 5 min) for urgent investigation

### Pydantic output models
- **Both layers**: structured Pydantic returns from crews AND validation before sending to citizens
- **All crews** get Pydantic output models — ManagerCrew, AuthCrew, MunicipalCrew, GBVCrew, TicketStatusCrew (all are citizen-facing)
- Fields: message (clean text) + language (en/zu/af) + metadata (action_taken, requires_followup, etc.)
- On validation failure: attempt repair from raw output first, if repair fails then use safe Gugu-voiced fallback message (with emergency numbers for GBV)

### System prompt engineering
- Internal reasoning and delegation leaked to citizens — this is the top critical issue (see test transcript below)
- Agent hallucinated "phone call verification" capability that doesn't exist
- **Hard block in prompts**: explicitly list available tools, say "do NOT mention any other methods"
- Agent YAML does not list tools — another best practice deviation to fix
- **Per-crew strictness + universal guardrails**: all crews get banned patterns, max length, required elements; GBV ultra-strict (trauma-informed), auth structured, municipal more flexible
- Internal agent-to-agent delegation hidden via **code-level filtering** in crew_server.py (strip delegation text before returning to citizen)
- **Extensive, thorough CrewAI best practices research required** — scrutinize every existing crew implementation against best practices, fix all deviations, no more regressions
- Both prompt fixes and tool execution fixes are **equally urgent** — fix together, test as a whole

### Test suite
- Both unit tests (with mocks) AND integration tests (with mocks) — full crew flows mocked end-to-end
- Mock data: canned realistic Gugu responses PLUS deliberately bad/adversarial outputs (artifacts, wrong language, missing fields) to test validation and sanitization
- Pydantic validation tested implicitly as part of integration flows (not standalone schema tests)
- Existing Phase 6.9 tests: Claude's discretion on which need updating vs leaving as regression baseline

### Claude's Discretion
- Exact Pydantic model field names and types for metadata
- Which existing 6.9 tests need updating vs preserving as-is
- Specific regex patterns for artifact stripping
- Mock response content for test scenarios
- Repair strategy implementation details

</decisions>

<specifics>
## Specific Ideas

**Critical test transcript showing multiple failures (from Streamlit testing):**

The manager agent leaked its entire internal delegation protocol to the citizen as a response:
- "As the Municipal Services Manager, here is the complete and correct procedure for you, Gugu, to follow: Step 1..." — this was shown TO the citizen
- Agent offered "phone call verification" which is a hallucinated capability (no such tool exists)
- SMS OTP was never received despite agent claiming to send it multiple times
- Agent narrated internal routing steps instead of just responding naturally
- The citizen typed "im confused" because they saw internal agent instructions

**Key quality bar:** A citizen should NEVER see anything that looks like internal instructions, delegation steps, role descriptions, or agent-to-agent communication. Every response should read like a friendly human assistant wrote it.

**Research mandate:** This phase's research must be the most thorough CrewAI best practices audit yet — every crew, every agent YAML, every task YAML, every tool definition scrutinized against official CrewAI documentation and patterns.

</specifics>

<deferred>
## Deferred Ideas

None — discussion stayed within phase scope

</deferred>

---

*Phase: 06.9.1-fix-agent-output-formatting-pydantic-models-auth-otp-tool-failures-and-system-prompt-engineering*
*Context gathered: 2026-02-19*
