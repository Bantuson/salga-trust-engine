---
phase: 10.3-crewai-agent-rebuild-and-llm-evaluation-framework
plan: "09"
subsystem: eval-framework
tags: [playwright, claude-judge, eval, streamlit, automation]
dependency_graph:
  requires:
    - 10.3-08  # Integration tests + LLM fix (agents must work before Playwright eval)
  provides:
    - playwright_judge_engine    # PlaywrightJudge class for Streamlit-driven evals
    - playwright_eval_cli        # run_playwright_eval.py CLI runner
  affects:
    - tests/evals/               # New files added
tech_stack:
  added: []
  patterns:
    - "Playwright MCP + Claude-as-judge eval loop"
    - "Callback injection pattern (send/read/reset fns provided by Claude Code)"
    - "metadata_only flag for SEC-05/POPIA GBV report stripping"
key_files:
  created:
    - tests/evals/playwright_judge.py
    - tests/evals/run_playwright_eval.py
  modified:
    - tests/evals/conftest.py
decisions:
  - "tools_called passed as list[str] to format_rubric (not string) — fixed per judge_rubrics API"
  - "GBV conversation content stripped in run_scenario (not post-hoc) — SEC-05 compliance at source"
  - "PlaywrightJudge uses callback injection so eval engine is testable independently of Playwright"
metrics:
  duration: "2 minutes"
  completed_date: "2026-02-26"
  tasks_completed: 1
  tasks_total: 2
  files_created: 2
  files_modified: 1
requirements:
  - AI-05
  - AI-06
  - AI-07
---

# Phase 10.3 Plan 09: Playwright+Claude-judge Eval Engine Summary

**One-liner:** Playwright+Claude-as-judge eval engine driving Streamlit conversations dynamically, with rubric scoring and metadata-only GBV report compliance.

## What Was Built

### Task 1: Playwright+Claude-judge eval engine and CLI runner (COMPLETE — commit 84770be)

Three files created/updated:

**`tests/evals/playwright_judge.py`** — Core eval engine containing `PlaywrightJudge` class:
- `run_scenario()`: Resets session, sends message, waits for response, scores via rubric
- `_score_response()`: Formats rubric prompt from `judge_rubrics.py`; Claude IS the judge when running interactively
- `run_agent_eval()`: Iterates all scenarios for a given agent, saves timestamped JSON report
- `_save_report()`: Writes to `tests/evals/reports/{agent}_playwright_{timestamp}.json`
- GBV `metadata_only=True` scenarios have conversation content stripped at `run_scenario()` level (SEC-05/POPIA)
- `SCENARIO_MAP` exports all 4 agent scenario sets (25 total scenarios)

**`tests/evals/run_playwright_eval.py`** — CLI entry point:
- `--agent [auth|municipal|ticket_status|gbv|all]` — target agent selection
- `--dry-run` — lists all scenarios with tool sequences and metadata flags; no API keys required
- `--crew-server-url` / `--streamlit-url` — configurable server URLs
- Documents Claude Code's Playwright MCP workflow for live eval runs

**`tests/evals/conftest.py`** — Added two fixtures:
- `playwright_judge` — creates `PlaywrightJudge()` instance for pytest usage
- `eval_report_dir` — returns `Path` to `tests/evals/reports/`, creating it if needed

### Task 2: Run Playwright+Claude-judge eval loop against live agents (AWAITING CHECKPOINT)

This task requires:
1. Starting `crew_server` on port 8001
2. Starting Streamlit on port 8501
3. Claude Code using Playwright MCP to drive the eval loop
4. Real `DEEPSEEK_API_KEY` and `OPENAI_API_KEY` for live agent responses

## Verification Results

```
$ python -c "from tests.evals.playwright_judge import PlaywrightJudge, SCENARIO_MAP; ..."
PlaywrightJudge created, agents: ['auth', 'municipal', 'ticket_status', 'gbv']

$ python -m tests.evals.run_playwright_eval --agent all --dry-run
Dry-run mode -- listing Playwright+Claude-judge scenarios
[auth] 7 scenarios
[municipal] 7 scenarios
[ticket_status] 6 scenarios
[gbv] 5 scenarios (SEC-05/POPIA: 5 metadata_only)
Total: 25 scenarios across 4 agent(s)
```

## Deviations from Plan

### Auto-fixed Issues

**1. [Rule 1 - Bug] Fixed tools_called type mismatch in _score_response**
- **Found during:** Task 1 implementation
- **Issue:** Plan code passed `"(captured from step_callback)"` (a string) to `format_rubric(tools_called=...)`, but `judge_rubrics.format_rubric` expects `list[str]` and joins the list itself
- **Fix:** Changed `_score_response` to extract `tools_called = scenario.get("expected_tool_sequence", [])` as a list, matching the `format_rubric` API signature
- **Files modified:** `tests/evals/playwright_judge.py`
- **Commit:** 84770be

## Self-Check: PASSED

| Item | Status |
|------|--------|
| `tests/evals/playwright_judge.py` | FOUND |
| `tests/evals/run_playwright_eval.py` | FOUND |
| Commit 84770be | FOUND |

## Status

**CHECKPOINT REACHED** — Task 1 complete, awaiting Task 2 human-verify checkpoint (live Playwright eval run against live agents).
