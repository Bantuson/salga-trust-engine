---
phase: 10.3-crewai-agent-rebuild-and-llm-evaluation-framework
plan: 04
type: execute
wave: 2
depends_on: ["10.3-01", "10.3-02"]
files_modified:
  - tests/agents/test_auth_crew.py
  - tests/agents/test_crew_server.py
autonomous: true
requirements:
  - AI-05
  - AI-07

must_haves:
  truths:
    - "Auth crew unit tests verify agent instantiation, YAML loading, and tool attachment"
    - "Auth crew unit tests verify Gugu persona is in prompts for all 3 languages"
    - "Auth crew unit tests verify AuthResult Pydantic model strips Final Answer prefix"
    - "Crew server unit tests verify /health returns 200"
    - "Crew server unit tests verify /chat returns ChatResponse shape"
    - "Crew server unit tests verify sanitize_reply strips internal artifacts"
  artifacts:
    - path: "tests/agents/test_auth_crew.py"
      provides: "Unit tests for AuthCrew"
      contains: "class TestAuthCrew"
    - path: "tests/agents/test_crew_server.py"
      provides: "Unit tests for rebuilt crew_server.py"
      contains: "class TestCrewServer"
  key_links:
    - from: "tests/agents/test_auth_crew.py"
      to: "src/agents/crews/auth_crew.py"
      via: "import and instantiation"
      pattern: "from src.agents.crews.auth_crew import AuthCrew"
    - from: "tests/agents/test_crew_server.py"
      to: "src/api/v1/crew_server.py"
      via: "TestClient HTTP calls"
      pattern: "TestClient\\(crew_app\\)"
---

<objective>
Write unit tests for the rebuilt Auth Agent and crew_server.py.

Purpose: The completion gate per agent requires automated tests to pass. These tests verify structure (agent instantiates, tools attach, YAML loads), behavior (prompts contain Gugu identity, AuthResult strips artifacts), and HTTP layer (crew_server endpoints return correct shapes). Tests use mocks — no real LLM calls — so they run fast and reliably in CI.

Output: Comprehensive unit tests for AuthCrew and crew_server, all passing
</objective>

<execution_context>
@C:/Users/Bantu/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Bantu/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/10.3-crewai-agent-rebuild-and-llm-evaluation-framework/10.3-RESEARCH.md
@.planning/phases/10.3-crewai-agent-rebuild-and-llm-evaluation-framework/10.3-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Auth agent unit tests</name>
  <files>
    tests/agents/test_auth_crew.py
  </files>
  <action>
    Create comprehensive unit tests for AuthCrew. All tests use mocks — no real LLM calls.

    **Section 1: Agent Structure Tests**
    - `test_auth_crew_instantiates` — AuthCrew() creates without error
    - `test_auth_crew_instantiates_with_language` — AuthCrew(language="zu") stores language
    - `test_auth_crew_uses_routing_llm` — Verify AuthCrew uses get_routing_llm (gpt-4o-mini), not DeepSeek
    - `test_auth_crew_create_crew_returns_crew` — create_crew(context) returns a Crew object
    - `test_auth_crew_has_correct_tools` — Agent has lookup_user_tool, send_otp_tool, verify_otp_tool, create_supabase_user_tool
    - `test_auth_crew_sequential_process` — Crew uses Process.sequential (not hierarchical)
    - `test_auth_crew_memory_disabled` — Crew has memory=False (PII protection per locked decision)
    - `test_auth_crew_no_delegation` — Agent has allow_delegation=False

    **Section 2: Prompt Tests**
    - `test_auth_prompts_all_languages` — AUTH_PROMPTS has "en", "zu", "af" keys
    - `test_auth_prompts_contain_gugu` — Each prompt contains "Gugu" identity
    - `test_auth_prompts_critical_identity_first` — Each prompt starts with CRITICAL IDENTITY section
    - `test_auth_prompts_contain_emergency_instruction` — No PII logging instruction present
    - `test_build_auth_task_new_user` — build_auth_task_description returns new user instructions when session_status="none"
    - `test_build_auth_task_returning_user` — build_auth_task_description returns returning user instructions when session_status="active"

    **Section 3: Pydantic Model Tests**
    - `test_auth_result_strips_final_answer` — AuthResult(message="Final Answer: Hello") → message="Hello"
    - `test_auth_result_preserves_clean_message` — AuthResult(message="Hello") → message="Hello"
    - `test_auth_result_defaults` — requires_otp=False, session_status="none", language="en"
    - `test_auth_result_custom_fields` — All fields accept custom values

    **Section 4: Tool Tests**
    - `test_all_auth_tools_importable` — All 4 tools import and have .name attribute
    - `test_lookup_user_tool_name` — lookup_user_tool.name == "lookup_user_tool" (or similar)
    - `test_send_otp_tool_name` — send_otp_tool.name verified
    - `test_verify_otp_tool_name` — verify_otp_tool.name verified

    **Mock pattern:**
    - Patch `src.agents.llm.get_routing_llm` to return mock_llm fixture
    - For create_crew tests, use fixture auth_context from conftest
    - For tool tests, only test importability and .name/.run attributes — actual Supabase calls tested separately

    **Important:** Set `os.environ["OPENAI_API_KEY"] = "fake-key"` before any CrewAI imports (already in conftest).
  </action>
  <verify>
    <automated>pytest tests/agents/test_auth_crew.py -x --no-header -q</automated>
  </verify>
  <done>18+ unit tests for AuthCrew covering structure (instantiation, tools, process, memory), prompts (all languages, Gugu identity, critical identity), Pydantic model (Final Answer stripping), and tool importability. All tests pass.</done>
</task>

<task type="auto">
  <name>Task 2: Crew server unit tests</name>
  <files>
    tests/agents/test_crew_server.py
  </files>
  <action>
    Create unit tests for the rebuilt crew_server.py using TestClient. All agent calls are mocked.

    **Section 1: Health Endpoint**
    - `test_health_returns_200` — GET /api/v1/health returns 200 with {"status": "ok"}
    - `test_health_lists_agents` — Response includes "agents" list

    **Section 2: Chat Endpoint Shape**
    - `test_chat_returns_chat_response_shape` — POST /api/v1/chat returns reply, agent_name, session_status, language, debug
    - `test_chat_requires_phone` — Missing phone returns 422
    - `test_chat_requires_message` — Missing message returns 422

    **Section 3: Chat Routing (mocked agents)**
    - `test_chat_routes_unauthenticated_to_auth` — When session_status="none", routes to auth agent
    - `test_chat_session_override_new` — session_override="new" forces session_status="none"

    **Section 4: sanitize_reply Tests**
    - `test_sanitize_strips_final_answer` — "Final Answer: Hello" → "Hello"
    - `test_sanitize_strips_thought` — "Thought: thinking...\nHello there" → "Hello there"
    - `test_sanitize_strips_action` — "Action: tool_name\nAction Input: {...}\nHello" → "Hello"
    - `test_sanitize_strips_delegation_artifacts` — "As the Authentication Specialist, I will help. Hello" → "Hello"
    - `test_sanitize_preserves_clean_text` — "Hello, I am Gugu" → unchanged
    - `test_sanitize_gbv_ensures_emergency_numbers` — For GBV agent, if emergency numbers missing, they are re-added
    - `test_sanitize_gbv_preserves_existing_numbers` — If 10111 already present, not duplicated

    **Section 5: Session Reset**
    - `test_session_reset_returns_200` — POST /api/v1/session/reset returns success

    **Section 6: Debug Output**
    - `test_debug_gbv_metadata_only` — For GBV agent, debug dict contains only metadata (no conversation content)
    - `test_debug_non_gbv_has_details` — For auth agent, debug dict includes routing_phase, detected_language

    **Mock pattern:**
    - Mock ConversationManager to return predictable state
    - Mock AuthCrew.kickoff to return predictable result dict
    - Use TestClient(crew_app) for HTTP tests
    - Import sanitize_reply directly for unit tests of that function
  </action>
  <verify>
    <automated>pytest tests/agents/test_crew_server.py -x --no-header -q</automated>
  </verify>
  <done>15+ unit tests for crew_server covering health endpoint, chat endpoint shape validation, routing logic, sanitize_reply (7 sanitization cases), session reset, and debug output (GBV metadata-only). All tests pass.</done>
</task>

</tasks>

<verification>
- `pytest tests/agents/test_auth_crew.py -x -q` — all tests pass
- `pytest tests/agents/test_crew_server.py -x -q` — all tests pass
- `pytest tests/agents/ -x -q` — full agent test suite green
</verification>

<success_criteria>
- 30+ unit tests across Auth agent and crew server
- No real LLM calls in any test
- All sanitize_reply edge cases covered (Final Answer, Thought, Action, delegation, GBV emergency numbers)
- Auth agent structure verified (sequential process, memory=False, correct tools, Gugu persona)
- Crew server HTTP layer verified (health, chat shape, routing, session reset)
- All tests pass in < 30 seconds
</success_criteria>

<output>
After completion, create `.planning/phases/10.3-crewai-agent-rebuild-and-llm-evaluation-framework/10.3-04-SUMMARY.md`
</output>
