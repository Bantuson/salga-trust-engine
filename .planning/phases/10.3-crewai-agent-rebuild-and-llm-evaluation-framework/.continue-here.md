---
phase: 10.3-crewai-agent-rebuild-and-llm-evaluation-framework
task: checkpoint-between-wave5-and-wave6
total_plans: 9
plans_complete: 7
status: in_progress
last_updated: 2026-02-25T18:39:24.478Z
---

<current_state>
Phase 10.3 execution is at Wave 5 checkpoint (Plan 10.3-08). Plans 01-07 are fully complete with SUMMARYs and commits. Plan 08 completed Task 1 (integration tests + eval runner) but Task 2 (manual Streamlit verification) hit an API key bug in crew_server.py, then the user changed the verification protocol — requesting per-agent Playwright+rubric evaluation instead of a simple manual smoke-test.

The user explicitly asked: "test each agent individually on Streamlit against rubric, do it yourself or spawn subagent to act as LLM-as-judge, create gitignored folder to store Playwright screen captures. After full agent run, determine if iteration is required or agent passes."

We started the Playwright eval process but hit a blocking bug before any agent could be tested.
</current_state>

<completed_work>

## Plans Complete (7/9)

- Plan 10.3-01: Archive & scaffold new agent architecture — DONE (3 commits)
- Plan 10.3-02: LLM evaluation framework (scenarios, rubrics, trajectory harness) — DONE (3 commits)
- Plan 10.3-03: Auth Agent rebuild + crew_server.py — DONE (3 commits)
- Plan 10.3-04: Auth Agent & crew_server unit tests (74 tests) — DONE (3 commits)
- Plan 10.3-05: Municipal Intake & Ticket Status agents + tests (88 tests) — DONE (2 commits)
- Plan 10.3-06: GBV Agent + SEC-05 boundary tests (58 tests) — DONE (3 commits)
- Plan 10.3-07: IntakeFlow router + full integration (35 tests) — DONE (3 commits)

Total: 274 tests passing in `tests/agents/` suite.

## Plan 10.3-08 Partial

- Task 1 DONE: `tests/agents/test_full_pipeline.py` (19 integration tests) + `tests/evals/run_evals.py` (CLI eval runner). Commit: `e58167b`
- Task 2 NOT DONE: Manual verification checkpoint — user changed protocol to per-agent Playwright eval

## Eval Infrastructure Created

- `tests/evals/screenshots/` directory created and added to `.gitignore`
- Servers were started (crew_server :8001, Streamlit :8501) — now stopped
</completed_work>

<remaining_work>

## Blocking Bug: crew_server.py OPENAI_API_KEY

`src/api/v1/crew_server.py` line 44:
```python
os.environ.setdefault("OPENAI_API_KEY", "dummy-key-for-crewai-validation")
```
This runs BEFORE Pydantic Settings loads `.env`, so the dummy key always wins. The real OPENAI_API_KEY from `.env` never reaches `os.environ`. Fix: add `from dotenv import load_dotenv; load_dotenv()` before the setdefault, or use `settings.OPENAI_API_KEY` to populate `os.environ` after settings load.

## Per-Agent Playwright Eval (User's New Protocol)

For each of the 4 agents (auth, municipal, ticket_status, gbv):
1. Start crew_server + Streamlit
2. Navigate Playwright to Streamlit dashboard
3. Set appropriate session_override (new/active/expired)
4. Send test messages matching eval scenarios from `tests/evals/scenarios/`
5. Capture screenshots to `tests/evals/screenshots/`
6. Evaluate each response against rubric from `tests/evals/judge_rubrics.py`
7. Determine PASS/FAIL per agent, decide if iteration needed

## Remaining Plans

- Plan 10.3-08 Task 2: Complete with per-agent Playwright eval (replaces simple smoke-test)
- Plan 10.3-09: Playwright+Claude-as-judge automated eval loop (may be partially satisfied by the manual eval work)

## Phase Completion Steps (after all plans done)

- Phase verification (gsd-verifier)
- ROADMAP update
- Close parent UAT artifacts (decimal phase 10.3 → parent 10)
</remaining_work>

<decisions_made>

- Prompts in Python files (not YAML backstory) — DELIBERATE for trilingual support. Each backstory is 30-50 lines × 3 languages. Documented in auth_crew.py lines 19-21.
- User changed checkpoint protocol: instead of simple "approved/tests-only", user wants per-agent Playwright testing against rubrics with screenshots. This is MORE thorough than the original plan.
- Auth agent uses gpt-4o-mini (not DeepSeek) — locked decision for reliable multi-tool chaining
- GBV agent uses DeepSeek — conversation-heavy, minimal tool use
</decisions_made>

<blockers>

- **crew_server.py API key bug**: `os.environ.setdefault("OPENAI_API_KEY", "dummy-key")` overrides real key from `.env`. Must fix before any live agent testing. Quick fix: add `from dotenv import load_dotenv; load_dotenv()` as first line of crew_server.py, or after settings import do `os.environ["OPENAI_API_KEY"] = settings.OPENAI_API_KEY`.
</blockers>

<context>
The user wants quality verification BEFORE continuing to Plan 09. The rubrics in `tests/evals/judge_rubrics.py` define pass/fail criteria per agent (persona, language, tool_sequence, no_leakage, etc.). The eval scenarios in `tests/evals/scenarios/*.py` have 25 total scenarios across 4 agents.

The Playwright MCP tools work well with Streamlit — we successfully navigated, filled forms, and sent messages. The only blocker was the API key bug preventing real LLM responses.
</context>

<next_action>
1. Fix the crew_server.py API key bug (add dotenv load or settings-based env population)
2. Restart crew_server + Streamlit
3. Test Auth Agent via Playwright (new user greeting scenario)
4. Evaluate response against AUTH_JUDGE_RUBRIC
5. Screenshot and record pass/fail
6. Repeat for Municipal, Ticket Status, GBV agents
7. If all pass: complete Plan 08, proceed to Plan 09
8. If any fail: iterate on that agent's implementation
</next_action>
