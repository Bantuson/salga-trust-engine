---
phase: 10.3-crewai-agent-rebuild-and-llm-evaluation-framework
task: switch-failing-agents-to-deepseek
total_plans: 9
plans_complete: 7
status: in_progress
last_updated: 2026-02-26T00:00:00.000Z
---

<current_state>
Phase 10.3 Wave 5 checkpoint. Plans 01-07 complete. Plan 08 Task 1 done.

Per-agent eval: 1/4 PASS (GBV/DeepSeek), 3/4 FAIL (Auth/Municipal/TicketStatus on gpt-4o-mini).

Diagnosis complete: prompts are well-written (150-300 lines with explicit tool instructions,
step-by-step flows, CRITICAL IDENTITY anchors). gpt-4o-mini is too weak for long complex
system prompts + tool use. DeepSeek handles this better (proven by GBV passing).
</current_state>

<fix_plan>

## Fix: Switch 3 failing agents from gpt-4o-mini to DeepSeek

The research doc's locked decision was "gpt-4o-mini for tool-heavy agents" but eval proves
this doesn't work — gpt-4o-mini ignores backstory and tools with long prompts.

### Files to change (3 crew files):

1. `src/agents/crews/auth_crew.py` line 61:
   Change: `from src.agents.llm import get_routing_llm` → `get_deepseek_llm`
   Change: `super().__init__(language=language, llm=llm or get_routing_llm())` → `get_deepseek_llm()`
   Update docstring to explain why DeepSeek (backstory compliance > tool reliability)

2. `src/agents/crews/municipal_crew.py`:
   Same pattern — switch from get_routing_llm() to get_deepseek_llm()

3. `src/agents/crews/ticket_status_crew.py`:
   Same pattern — switch from get_routing_llm() to get_deepseek_llm()

### Keep gpt-4o-mini ONLY for:
- `src/agents/flows/intake_flow.py` — intent classification (short prompt, no tools, works fine)

### After switching:
1. Re-run all 4 agent evals (same test commands from previous session)
2. If all pass: complete Plan 08, proceed to Plan 09
3. Update unit tests if any mock the LLM factory function name

### Test commands:
```bash
# Start server
uvicorn src.api.v1.crew_server:crew_app --port 8001 --host 127.0.0.1

# Test auth (via API)
curl -s -X POST http://localhost:8001/api/v1/session/reset -H "Content-Type: application/json" -d '{"phone": "+27821234567"}'
curl -s -X POST http://localhost:8001/api/v1/chat -H "Content-Type: application/json" -d '{"phone": "+27821234567", "message": "Hello, I want to register", "language": "en"}'

# Test municipal (direct crew call)
python -c "
import os, asyncio, json
from dotenv import load_dotenv; load_dotenv()
os.environ.setdefault('OPENAI_API_KEY', 'dummy')
from src.agents.crews.municipal_crew import MunicipalIntakeCrew
async def test():
    crew = MunicipalIntakeCrew(language='en')
    result = await crew.kickoff({'phone': '+27821234567', 'language': 'en', 'message': 'Water pipe burst on Main Street, Braamfontein', 'conversation_history': '(none)', 'user_id': 'test-user-123', 'municipality_id': 'test-muni'})
    print(json.dumps(result, indent=2, default=str))
asyncio.run(test())
"

# Test ticket status (direct crew call)
# Same pattern, use TicketStatusCrew with message "Check my report TKT-20260220-A1B2C3"

# Test GBV (direct crew call) — should still pass
# Same pattern, use GBVCrew — only log metadata per SEC-05
```
</fix_plan>

<next_action>
1. Edit 3 crew files to use get_deepseek_llm() instead of get_routing_llm()
2. Update docstrings explaining the LLM choice change
3. Run unit tests to check nothing broke (274 tests)
4. Re-run live eval for all 4 agents
5. If all pass: complete Plan 08, proceed to Plan 09
</next_action>
