# Phase 10.3: CrewAI Agent Rebuild and LLM Evaluation Framework - Context

**Gathered:** 2026-02-25
**Status:** Ready for planning

<domain>
## Phase Boundary

Rebuild the entire CrewAI agent system from scratch — archive existing code, rebuild one agent at a time with proper CrewAI understanding, fully test each agent end-to-end before moving to the next. Establish an LLM evaluation framework using trajectory evals + Claude Code as LLM judge via Playwright/Streamlit. The goal is agents that don't regress.

**Root cause being addressed:** CrewAI Flows and Crews not properly understood, causing conversation-heavy architecture to break under framework constraints. Traditional software testing inadequate for LLM agent quality.

</domain>

<decisions>
## Implementation Decisions

### Rebuild Sequence
- **Agent order:** Auth Agent → Municipal Intake Agent → Ticket Status Agent → GBV Agent → Manager Agent
- Auth first because it has concrete tool calls (send OTP, verify, lookup) with clear success/fail states — most testable
- Specialists built independently before Manager — each proven in isolation
- Manager wired last to route between known-good agents
- **Completion gate per agent:** Must pass automated eval suite AND manual spot-check via Streamlit before moving to next agent
- **Clean slate approach:** Archive current `src/agents/` to `src/agents_old/` (or branch). Start with empty `src/agents/` and build from zero

### Architecture Approach
- **Flows vs Crews:** Research first — deep-dive CrewAI docs before committing to pure Crews or Flows. Current Flows create constraints that break the system; need to understand if this is fixable or if Flows should be dropped
- **Conversation state management:** Decide after research — let CrewAI docs determine whether Redis session + Python dict (current approach) or CrewAI's built-in memory features are better for multi-turn
- **Crew server:** Rebuild crew_server.py from scratch alongside the first agent (Auth). Full HTTP-to-agent-to-response path must be proven end-to-end
- **LLM provider:** Keep DeepSeek for now (specialist agents). Switch to GPT-4o-mini when Manager agent is built for routing — DeepSeek struggles with tool use
- **LLM provider note:** DeepSeek for conversation/specialist work, 4o-mini for Manager routing decisions where tool use reliability is critical

### Eval Framework
- **Eval types:** Trajectory evals (tool call correctness) + LLM-as-judge (Claude Code via Playwright)
- **Trajectory evals:** Define expected tool calls and routing decisions per conversation scenario. E.g., Auth agent must call send_otp_tool after collecting phone. Checks ACTION sequence, not exact wording
- **LLM judge:** Claude Code drives Playwright live against Streamlit — sends messages, reads responses, decides next input based on agent behavior. Fully dynamic, replaces manual human verification
- **Judge criteria:** Rubric-based scoring per agent — stayed in character (Gugu persona)? Asked for required info? Used correct tool? Responded in right language? Each criterion scored pass/fail
- **Scenarios per agent:** 5-7 core scenarios — happy path + 2-3 edge cases + 1 language switch + 1 adversarial input
- **Eval result storage:** Timestamped report files (JSON or Markdown). Diff results across runs to catch regressions

### Testing Workflow
- **Streamlit:** Keep current Streamlit dev server as-is — it works for conversations, just point it at rebuilt crew_server
- **Playwright eval loop:** Claude Code uses Playwright MCP to interact with Streamlit in real-time. Dynamic conversation — not scripted, adapts to agent responses
- **Eval cadence:** Evals run as completion gate after each agent rebuild. During development, use manual Streamlit testing
- **Integration eval:** Full system eval at the end — citizen messages hit Manager, get routed to correct specialist, full conversation completes. Ultimate gate before phase is done

### Claude's Discretion
- Exact CrewAI architecture (Flows vs pure Crews) — pending research outcome
- Conversation state approach (Redis vs CrewAI memory) — pending research
- Eval report format details (JSON structure, Markdown template)
- How to structure the archive of old agent code
- Specific rubric criteria per agent (derived from agent responsibilities)

</decisions>

<specifics>
## Specific Ideas

- "DeepSeek really struggles with tool use" — this is the driver for 4o-mini on Manager agent routing
- The regression cycle (improve → regress → improve → regress) indicates prompts and framework usage are fighting each other
- "Key flaw in implementation is not understanding CrewAI flows and crews properly" — research phase must deeply cover Flows vs Crews, when to use each, conversation patterns
- Claude Code as judge replaces manual human verification — the Streamlit + Playwright + live Claude Code loop is the core eval innovation
- Research should cover how experts develop evals for conversational AI agents — this is not traditional software testing

</specifics>

<deferred>
## Deferred Ideas

None — discussion stayed within phase scope

</deferred>

---

*Phase: 10.3-crewai-agent-rebuild-and-llm-evaluation-framework*
*Context gathered: 2026-02-25*
