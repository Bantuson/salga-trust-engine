---
phase: 10.3-crewai-agent-rebuild-and-llm-evaluation-framework
plan: 09
type: execute
wave: 6
depends_on: ["10.3-08"]
files_modified:
  - tests/evals/playwright_judge.py
  - tests/evals/run_playwright_eval.py
  - tests/evals/conftest.py
autonomous: false
requirements:
  - AI-05
  - AI-06
  - AI-07

must_haves:
  truths:
    - "Playwright eval script starts crew server, opens Streamlit, and runs dynamic conversations"
    - "Claude-as-judge scores each agent response against rubric criteria from judge_rubrics.py"
    - "Eval loop is dynamic — Claude decides next input based on agent behavior, not scripted"
    - "Results saved to tests/evals/reports/ as timestamped JSON with pass/fail per criterion"
    - "GBV eval reports contain metadata only — never conversation content"
    - "All 4 specialist agents can be evaluated via the Playwright loop"
  artifacts:
    - path: "tests/evals/playwright_judge.py"
      provides: "Playwright+Claude-judge eval engine that drives Streamlit conversations and scores responses"
      contains: "PlaywrightJudge"
    - path: "tests/evals/run_playwright_eval.py"
      provides: "CLI entry point for running Playwright+Claude-judge eval loop"
      contains: "main"
  key_links:
    - from: "tests/evals/playwright_judge.py"
      to: "tests/evals/judge_rubrics.py"
      via: "import RUBRIC_MAP and format_rubric for scoring"
      pattern: "from tests.evals.judge_rubrics import"
    - from: "tests/evals/playwright_judge.py"
      to: "tests/evals/scenarios/"
      via: "import scenario definitions for each agent"
      pattern: "from tests.evals.scenarios"
    - from: "tests/evals/run_playwright_eval.py"
      to: "tests/evals/playwright_judge.py"
      via: "import PlaywrightJudge"
      pattern: "PlaywrightJudge"
---

<objective>
Implement the Playwright+Claude-as-judge evaluation loop that dynamically tests agents via Streamlit.

Purpose: This is the locked decision from CONTEXT.md: "Claude Code drives Playwright live against Streamlit — sends messages, reads responses, decides next input based on agent behavior. Fully dynamic, replaces manual human verification." This plan creates the automation that replaces Plan 08's manual Streamlit checkpoint with a reproducible, automated eval loop. The eval script starts the crew server, opens Streamlit via Playwright, runs each scenario from tests/evals/scenarios/, applies judge_rubrics.py scoring, and saves results to tests/evals/reports/.

Output: Automated Playwright+Claude-judge eval loop with CLI runner, scoring against rubrics, timestamped report generation
</objective>

<execution_context>
@C:/Users/Bantu/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Bantu/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/10.3-crewai-agent-rebuild-and-llm-evaluation-framework/10.3-RESEARCH.md
@.planning/phases/10.3-crewai-agent-rebuild-and-llm-evaluation-framework/10.3-02-SUMMARY.md
@.planning/phases/10.3-crewai-agent-rebuild-and-llm-evaluation-framework/10.3-08-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Build Playwright+Claude-judge eval engine and CLI runner</name>
  <files>
    tests/evals/playwright_judge.py
    tests/evals/run_playwright_eval.py
    tests/evals/conftest.py
  </files>
  <action>
    Build the automated eval loop that replaces manual Streamlit verification. This uses the Playwright MCP tools (browser_navigate, browser_snapshot, browser_click, browser_type, browser_wait_for) that are already configured in the project.

    1. `tests/evals/playwright_judge.py` — Core eval engine:

       ```python
       """Playwright+Claude-judge eval engine.

       Drives Streamlit conversations via Playwright MCP and scores responses
       against judge rubrics. This is the automated replacement for manual
       Streamlit verification per CONTEXT.md locked decision.

       Usage: Called by run_playwright_eval.py CLI or directly by Claude Code.
       """
       import json
       import subprocess
       import time
       import os
       from datetime import datetime
       from typing import Callable

       from tests.evals.judge_rubrics import RUBRIC_MAP, format_rubric
       from tests.evals.scenarios.auth_scenarios import AUTH_SCENARIOS
       from tests.evals.scenarios.municipal_scenarios import MUNICIPAL_SCENARIOS
       from tests.evals.scenarios.ticket_status_scenarios import TICKET_STATUS_SCENARIOS
       from tests.evals.scenarios.gbv_scenarios import GBV_SCENARIOS

       SCENARIO_MAP = {
           "auth": AUTH_SCENARIOS,
           "municipal": MUNICIPAL_SCENARIOS,
           "ticket_status": TICKET_STATUS_SCENARIOS,
           "gbv": GBV_SCENARIOS,
       }

       class PlaywrightJudge:
           """Orchestrates Playwright-driven eval loop with Claude-as-judge scoring.

           Design:
           - Each eval session: reset session → send scenario input → read response → score → next input
           - Claude decides next input dynamically based on agent response (not scripted)
           - Scoring uses rubric from judge_rubrics.py
           - GBV scenarios: metadata-only reporting (no conversation content in reports)
           """

           def __init__(self, crew_server_url: str = "http://localhost:8001",
                        streamlit_url: str = "http://localhost:8501"):
               self.crew_server_url = crew_server_url
               self.streamlit_url = streamlit_url
               self.results = []
               self.conversation_history = []

           def run_scenario(self, agent_name: str, scenario: dict,
                           send_message_fn: Callable,
                           read_response_fn: Callable,
                           reset_session_fn: Callable) -> dict:
               """Run a single eval scenario via Playwright.

               Args:
                   agent_name: Which agent is being evaluated
                   scenario: Scenario dict from scenarios/*.py
                   send_message_fn: Callable that sends a message in Streamlit via Playwright
                       Signature: (message: str) -> None
                   read_response_fn: Callable that reads the latest agent response from Streamlit
                       Signature: () -> str
                   reset_session_fn: Callable that resets the Streamlit session
                       Signature: () -> None

               Returns:
                   dict with scenario_name, criteria scores, pass/fail, conversation metadata
               """
               # Step 1: Reset session
               reset_session_fn()
               self.conversation_history = []

               # Step 2: Send initial message
               send_message_fn(scenario["input"])
               self.conversation_history.append({"role": "user", "content": scenario["input"]})

               # Step 3: Wait for and read response
               time.sleep(5)  # Allow LLM to respond; Claude can adjust dynamically
               response = read_response_fn()
               self.conversation_history.append({"role": "assistant", "content": response})

               # Step 4: Score response against rubric
               score_result = self._score_response(
                   agent_name=agent_name,
                   response=response,
                   scenario=scenario,
               )

               # Step 5: Build result (metadata-only for GBV)
               result = {
                   "scenario_name": scenario["name"],
                   "agent_name": agent_name,
                   "timestamp": datetime.utcnow().isoformat(),
                   "criteria": score_result.get("criteria", {}),
                   "pass": score_result.get("pass", False),
                   "reason": score_result.get("reason", ""),
                   "turn_count": len(self.conversation_history),
                   "language": scenario.get("language", "en"),
               }

               # GBV: strip conversation content from result
               if not scenario.get("metadata_only", False):
                   result["conversation"] = self.conversation_history
                   result["response_preview"] = response[:200]

               return result

           def _score_response(self, agent_name: str, response: str, scenario: dict) -> dict:
               """Score an agent response against the judge rubric.

               This method formats the rubric prompt and returns a structured
               score. When run by Claude Code, Claude itself IS the judge —
               it reads the formatted rubric and response, then returns
               pass/fail per criterion.

               In automated mode, this returns a template for Claude to fill.
               In Claude Code interactive mode, Claude evaluates directly.
               """
               rubric = format_rubric(
                   agent_name=agent_name,
                   response=response,
                   history=self._format_history(),
                   language=scenario.get("language", "en"),
                   tools_called="(captured from step_callback)",
               )

               # Return the formatted rubric as the scoring template.
               # When Claude Code drives this, Claude reads the rubric and
               # provides the JSON score. For non-interactive runs, this
               # returns a placeholder that the caller must evaluate.
               return {
                   "rubric_prompt": rubric,
                   "criteria": {},
                   "pass": False,
                   "reason": "Awaiting Claude Code judge evaluation",
               }

           def _format_history(self) -> str:
               """Format conversation history for rubric injection."""
               lines = []
               for turn in self.conversation_history:
                   role = "Citizen" if turn["role"] == "user" else "Gugu"
                   lines.append(f"{role}: {turn['content']}")
               return "\n".join(lines)

           def run_agent_eval(self, agent_name: str,
                             send_message_fn: Callable,
                             read_response_fn: Callable,
                             reset_session_fn: Callable) -> dict:
               """Run all scenarios for a given agent.

               Returns:
                   Summary dict with total, passed, failed, results per scenario
               """
               scenarios = SCENARIO_MAP.get(agent_name, [])
               results = []

               for scenario in scenarios:
                   result = self.run_scenario(
                       agent_name=agent_name,
                       scenario=scenario,
                       send_message_fn=send_message_fn,
                       read_response_fn=read_response_fn,
                       reset_session_fn=reset_session_fn,
                   )
                   results.append(result)

               summary = {
                   "agent_name": agent_name,
                   "timestamp": datetime.utcnow().isoformat(),
                   "total_scenarios": len(results),
                   "passed": sum(1 for r in results if r["pass"]),
                   "failed": sum(1 for r in results if not r["pass"]),
                   "results": results,
               }

               # Save report
               self._save_report(agent_name, summary)
               return summary

           def _save_report(self, agent_name: str, summary: dict):
               """Save eval report to tests/evals/reports/."""
               reports_dir = os.path.join(
                   os.path.dirname(__file__), "reports"
               )
               os.makedirs(reports_dir, exist_ok=True)

               timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
               filename = f"{agent_name}_playwright_{timestamp}.json"
               filepath = os.path.join(reports_dir, filename)

               with open(filepath, "w") as f:
                   json.dump(summary, f, indent=2, default=str)

               print(f"Report saved: {filepath}")
       ```

    2. `tests/evals/run_playwright_eval.py` — CLI entry point:

       ```python
       """CLI entry point for Playwright+Claude-judge eval loop.

       Usage (run by Claude Code interactively):
           python -m tests.evals.run_playwright_eval --agent auth
           python -m tests.evals.run_playwright_eval --agent all
           python -m tests.evals.run_playwright_eval --agent auth --dry-run

       This script is designed to be run BY Claude Code, which:
       1. Starts crew_server and Streamlit (or verifies they're running)
       2. Uses Playwright MCP to open Streamlit
       3. Provides send_message_fn, read_response_fn, reset_session_fn
          via Playwright browser interactions
       4. Acts as the LLM judge, scoring each response against rubrics

       The eval loop is DYNAMIC — Claude reads agent responses and decides
       the next input based on behavior, not from a fixed script.
       """
       import argparse
       import sys

       from tests.evals.playwright_judge import PlaywrightJudge, SCENARIO_MAP


       def main():
           parser = argparse.ArgumentParser(
               description="Run Playwright+Claude-judge eval loop"
           )
           parser.add_argument(
               "--agent",
               choices=["auth", "municipal", "ticket_status", "gbv", "all"],
               required=True,
               help="Which agent to evaluate"
           )
           parser.add_argument(
               "--dry-run",
               action="store_true",
               help="Show scenarios and rubric info without running evals"
           )
           parser.add_argument(
               "--crew-server-url",
               default="http://localhost:8001",
               help="URL of the crew server"
           )
           parser.add_argument(
               "--streamlit-url",
               default="http://localhost:8501",
               help="URL of the Streamlit dashboard"
           )
           args = parser.parse_args()

           agents = list(SCENARIO_MAP.keys()) if args.agent == "all" else [args.agent]

           if args.dry_run:
               for agent_name in agents:
                   scenarios = SCENARIO_MAP[agent_name]
                   print(f"\n{agent_name}: {len(scenarios)} scenarios (Playwright+Claude judge)")
                   for s in scenarios:
                       metadata_flag = " [metadata-only]" if s.get("metadata_only") else ""
                       print(f"  - {s['name']} (lang={s.get('language', 'en')}){metadata_flag}")
               print("\nTo run evals, Claude Code should:")
               print("  1. Start crew_server: uvicorn src.api.v1.crew_server:crew_app --port 8001")
               print("  2. Start Streamlit: streamlit run streamlit_dashboard/app.py")
               print("  3. Use Playwright MCP to navigate to Streamlit")
               print("  4. Run this script without --dry-run")
               return

           # Interactive mode: Claude Code provides Playwright callbacks
           print("=" * 60)
           print("PLAYWRIGHT+CLAUDE-JUDGE EVAL LOOP")
           print("=" * 60)
           print()
           print("This script requires Claude Code to drive Playwright MCP.")
           print("Claude Code should:")
           print("  1. Navigate to Streamlit via browser_navigate")
           print("  2. Create send_message_fn using browser_type + browser_click")
           print("  3. Create read_response_fn using browser_snapshot")
           print("  4. Create reset_session_fn using browser_click on 'New Session'")
           print("  5. Call PlaywrightJudge.run_agent_eval() with these callbacks")
           print()
           print("For automated use, import PlaywrightJudge directly:")
           print("  from tests.evals.playwright_judge import PlaywrightJudge")
           print()

           judge = PlaywrightJudge(
               crew_server_url=args.crew_server_url,
               streamlit_url=args.streamlit_url,
           )

           for agent_name in agents:
               print(f"\nReady to evaluate: {agent_name}")
               print(f"Scenarios: {len(SCENARIO_MAP[agent_name])}")
               print("Waiting for Claude Code to provide Playwright callbacks...")
               # In practice, Claude Code imports PlaywrightJudge and drives
               # the eval loop directly using Playwright MCP tools.
               # This CLI serves as documentation and dry-run capability.


       if __name__ == "__main__":
           main()
       ```

    3. Update `tests/evals/conftest.py` — Add Playwright judge fixtures:
       - Add fixture `playwright_judge` that creates a PlaywrightJudge instance
       - Add fixture `eval_report_dir` that returns the reports directory path
       - Keep existing `tool_calls_captured` and `make_step_callback` fixtures

    Key design decisions:
    - The PlaywrightJudge class provides the eval engine; Claude Code provides the Playwright callbacks
    - When Claude Code runs the eval, IT is the judge — it reads rubric prompts and scores responses
    - send_message_fn, read_response_fn, reset_session_fn are injected by the caller (Claude Code using Playwright MCP)
    - This separation allows the eval engine to be tested independently of Playwright
    - GBV scenarios always have metadata_only=True — conversation content stripped from reports
    - Reports are timestamped JSON saved to tests/evals/reports/
    - The CLI supports --dry-run for listing scenarios without running
  </action>
  <verify>
    <automated>python -c "from tests.evals.playwright_judge import PlaywrightJudge, SCENARIO_MAP; j = PlaywrightJudge(); print(f'PlaywrightJudge created, agents: {list(SCENARIO_MAP.keys())}')" && python -m tests.evals.run_playwright_eval --agent all --dry-run</automated>
  </verify>
  <done>PlaywrightJudge eval engine created with run_scenario, run_agent_eval, rubric scoring, and metadata-only GBV reporting. CLI runner supports --dry-run for scenario listing. All imports succeed.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Run Playwright+Claude-judge eval loop against live agents</name>
  <action>
    Claude Code runs the automated Playwright+Claude-judge eval loop:
    1. Start crew server: `uvicorn src.api.v1.crew_server:crew_app --port 8001`
    2. Start Streamlit: `streamlit run streamlit_dashboard/app.py`
    3. Navigate to Streamlit via `browser_navigate` to http://localhost:8501
    4. For each agent scenario:
       - Reset session (click "New Session" in sidebar)
       - Send scenario input message via `browser_type` + `browser_click`
       - Wait for response via `browser_wait_for`
       - Read response via `browser_snapshot`
       - Score response against rubric criteria (Claude IS the judge)
       - Decide next input dynamically based on agent behavior
    5. Save results to tests/evals/reports/
    6. Verify eval reports exist and GBV reports contain no conversation content (metadata only)

    NOTE: Requires real DEEPSEEK_API_KEY and OPENAI_API_KEY for live testing.
  </action>
  <verify>User reviews eval reports in tests/evals/reports/ and types "approved" or describes issues</verify>
  <done>All agent eval reports generated. User approves agent quality based on Playwright+Claude-judge eval results. Pass/fail per rubric criterion documented in timestamped JSON reports.</done>
</task>

</tasks>

<verification>
- `python -c "from tests.evals.playwright_judge import PlaywrightJudge"` succeeds
- `python -m tests.evals.run_playwright_eval --agent all --dry-run` lists all scenarios
- Eval reports generated in tests/evals/reports/ after live run
- GBV eval reports contain metadata only
- Claude-as-judge scores match rubric criteria from judge_rubrics.py
</verification>

<success_criteria>
- PlaywrightJudge class implements the full eval loop: start -> send -> read -> score -> report
- CLI runner works in dry-run mode listing all scenarios
- Rubric scoring uses judge_rubrics.py criteria (persona, language, tool sequence, no leakage)
- GBV reports are metadata-only (no conversation content)
- Reports saved as timestamped JSON in tests/evals/reports/
- Claude Code can drive the eval loop via Playwright MCP tools
- User approves agent quality after reviewing eval results
</success_criteria>

<output>
After completion, create `.planning/phases/10.3-crewai-agent-rebuild-and-llm-evaluation-framework/10.3-09-SUMMARY.md`
</output>
